{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "6L492Y_lFkuM",
        "outputId": "8c31175c-0b1b-420f-cd9e-d0bf6bf2b299"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.5.2-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.3.0)\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.5.2\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.25.2)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.66.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.0.7)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (67.7.2)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.31.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2023.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m929.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7029 sha256=564c73022cba56c1c92d6353be1988ec49da97896d44157ea66023858376039d\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, outdated, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ogb\n",
            "Successfully installed littleutils-0.2.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 ogb-1.3.6 outdated-0.2.2\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2023.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.25.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n",
            "Installing collected packages: rdkit\n",
            "Successfully installed rdkit-2023.9.5\n"
          ]
        }
      ],
      "source": [
        " # per installare la libreria datasets. La libreria datasets fornisce un facile accesso a numerosi set di dati\n",
        "# per l'apprendimento automatico e l'elaborazione del linguaggio naturale.\n",
        "!pip install datasets\n",
        "\n",
        "# per installare la libreria torch_geometric. torch_geometric è una libreria che fornisce funzionalità per il machine learning\n",
        "# geometrico basato su PyTorch. È ampiamente usata per la manipolazione e l'analisi di dati strutturati come grafi.\n",
        "!pip install torch_geometric\n",
        "\n",
        "# per installare la libreria ogb. ogb è la libreria Open Graph Benchmark che fornisce un insieme standardizzato di set di dati e\n",
        "# metriche per valutare le prestazioni degli algoritmi di apprendimento automatico su grafi.\n",
        "!pip install -U ogb\n",
        "\n",
        "# per installare la libreria rdkit. rdkit è una collezione di strumenti per la manipolazione\n",
        "#e l'analisi di dati chimici e strutture molecolari. È ampiamente utilizzata nell'ambito della chimica computazionale e del drug discovery.\n",
        "!pip install rdkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHQqBZsBiw4y"
      },
      "outputs": [],
      "source": [
        "ataset = PygGraphPropPredDataset(name='ogbg-molhiv')\n",
        "split_idx = dataset.get_idx_split()\n",
        "train_idx, val_idx, test_idx=split_idx['train'], split_idx['valid'], split_idx['test']\n",
        "train_data = dataset[train_idx]\n",
        "val_data = dataset[val_idx]\n",
        "test_data = dataset[test_idx]\n",
        "\n",
        "label_0_indices_tr = (train_data.y == 0).nonzero(as_tuple=True)[0].reshape(-1)\n",
        "label_1_indices_tr = (train_data.y == 1).nonzero(as_tuple=True)[0].reshape(-1)\n",
        "label_0_indices_va = (val_data.y == 0).nonzero(as_tuple=True)[0].reshape(-1)\n",
        "label_1_indices_va = (val_data.y == 1).nonzero(as_tuple=True)[0].reshape(-1)\n",
        "label_0_indices_te = (test_data.y == 0).nonzero(as_tuple=True)[0].reshape(-1)\n",
        "label_1_indices_te = (test_data.y == 1).nonzero(as_tuple=True)[0].reshape(-1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhVSyitxXiCW"
      },
      "source": [
        "#GCN intero\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4JH-kmNpHcr",
        "outputId": "7f4f5850-2be8-46a0-8ce6-80d455c54fb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://snap.stanford.edu/ogb/data/graphproppred/csv_mol_download/hiv.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloaded 0.00 GB: 100%|██████████| 3/3 [00:04<00:00,  1.35s/it]\n",
            "Processing...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting dataset/hiv.zip\n",
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 41127/41127 [00:00<00:00, 85006.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 41127/41127 [00:01<00:00, 21853.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Done!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold [1/5]\n",
            "Fold [1/5] Epoch: 1, Val Acc: 0.5976\n",
            "Fold [1/5] Epoch: 2, Val Acc: 0.6448\n",
            "Fold [1/5] Epoch: 3, Val Acc: 0.6145\n",
            "Fold [1/5] Epoch: 4, Val Acc: 0.5724\n",
            "Fold [1/5] Epoch: 5, Val Acc: 0.6633\n",
            "Fold [1/5] Epoch: 6, Val Acc: 0.6481\n",
            "Fold [1/5] Epoch: 7, Val Acc: 0.6549\n",
            "Fold [1/5] Epoch: 8, Val Acc: 0.6902\n",
            "Fold [1/5] Epoch: 9, Val Acc: 0.6700\n",
            "Fold [1/5] Epoch: 10, Val Acc: 0.6734\n",
            "Fold [1/5] Epoch: 11, Val Acc: 0.6751\n",
            "Fold [1/5] Epoch: 12, Val Acc: 0.6869\n",
            "Fold [1/5] Epoch: 13, Val Acc: 0.6296\n",
            "Fold [1/5] Epoch: 14, Val Acc: 0.6700\n",
            "Fold [1/5] Epoch: 15, Val Acc: 0.6684\n",
            "Fold [1/5] Epoch: 16, Val Acc: 0.6835\n",
            "Fold [1/5] Epoch: 17, Val Acc: 0.6566\n",
            "Fold [1/5] Epoch: 18, Val Acc: 0.6582\n",
            "Fold [1/5] Epoch: 19, Val Acc: 0.6886\n",
            "Fold [1/5] Epoch: 20, Val Acc: 0.6852\n",
            "Fold [1/5] Epoch: 21, Val Acc: 0.6212\n",
            "Fold [1/5] Epoch: 22, Val Acc: 0.6768\n",
            "Fold [1/5] Epoch: 23, Val Acc: 0.6886\n",
            "Fold [1/5] Epoch: 24, Val Acc: 0.6734\n",
            "Fold [1/5] Epoch: 25, Val Acc: 0.6616\n",
            "Fold [1/5] Epoch: 26, Val Acc: 0.6953\n",
            "Fold [1/5] Epoch: 27, Val Acc: 0.6886\n",
            "Fold [1/5] Epoch: 28, Val Acc: 0.6970\n",
            "Fold [1/5] Epoch: 29, Val Acc: 0.6734\n",
            "Fold [1/5] Epoch: 30, Val Acc: 0.6801\n",
            "Fold [1/5] Epoch: 31, Val Acc: 0.6835\n",
            "Fold [1/5] Epoch: 32, Val Acc: 0.6145\n",
            "Fold [1/5] Epoch: 33, Val Acc: 0.6094\n",
            "Fold [1/5] Epoch: 34, Val Acc: 0.6835\n",
            "Fold [1/5] Epoch: 35, Val Acc: 0.6936\n",
            "Fold [1/5] Epoch: 36, Val Acc: 0.6886\n",
            "Fold [1/5] Epoch: 37, Val Acc: 0.6987\n",
            "Fold [1/5] Epoch: 38, Val Acc: 0.7172\n",
            "Fold [1/5] Epoch: 39, Val Acc: 0.6886\n",
            "Fold [1/5] Epoch: 40, Val Acc: 0.7037\n",
            "Fold [1/5] Epoch: 41, Val Acc: 0.7138\n",
            "Fold [1/5] Epoch: 42, Val Acc: 0.7037\n",
            "Fold [1/5] Epoch: 43, Val Acc: 0.7071\n",
            "Fold [1/5] Epoch: 44, Val Acc: 0.6919\n",
            "Fold [1/5] Epoch: 45, Val Acc: 0.6667\n",
            "Fold [1/5] Epoch: 46, Val Acc: 0.7054\n",
            "Fold [1/5] Epoch: 47, Val Acc: 0.7222\n",
            "Fold [1/5] Epoch: 48, Val Acc: 0.6768\n",
            "Fold [1/5] Epoch: 49, Val Acc: 0.7104\n",
            "Fold [1/5] Epoch: 50, Val Acc: 0.6414\n",
            "Fold [1/5] Epoch: 51, Val Acc: 0.6801\n",
            "Fold [1/5] Epoch: 52, Val Acc: 0.7323\n",
            "Fold [1/5] Epoch: 53, Val Acc: 0.7323\n",
            "Fold [1/5] Epoch: 54, Val Acc: 0.7340\n",
            "Fold [1/5] Epoch: 55, Val Acc: 0.7424\n",
            "Fold [1/5] Epoch: 56, Val Acc: 0.7525\n",
            "Fold [1/5] Epoch: 57, Val Acc: 0.7576\n",
            "Fold [1/5] Epoch: 58, Val Acc: 0.7340\n",
            "Fold [1/5] Epoch: 59, Val Acc: 0.7273\n",
            "Fold [1/5] Epoch: 60, Val Acc: 0.7189\n",
            "Fold [1/5] Epoch: 61, Val Acc: 0.7576\n",
            "Fold [1/5] Epoch: 62, Val Acc: 0.7121\n",
            "Fold [1/5] Epoch: 63, Val Acc: 0.7104\n",
            "Fold [1/5] Epoch: 64, Val Acc: 0.7424\n",
            "Fold [1/5] Epoch: 65, Val Acc: 0.7155\n",
            "Fold [1/5] Epoch: 66, Val Acc: 0.6970\n",
            "Fold [1/5] Epoch: 67, Val Acc: 0.7189\n",
            "Fold [1/5] Epoch: 68, Val Acc: 0.7441\n",
            "Fold [1/5] Epoch: 69, Val Acc: 0.7357\n",
            "Fold [1/5] Epoch: 70, Val Acc: 0.7290\n",
            "Fold [1/5] Epoch: 71, Val Acc: 0.6835\n",
            "Fold [1/5] Epoch: 72, Val Acc: 0.7643\n",
            "Fold [1/5] Epoch: 73, Val Acc: 0.7424\n",
            "Fold [1/5] Epoch: 74, Val Acc: 0.7222\n",
            "Fold [1/5] Epoch: 75, Val Acc: 0.7205\n",
            "Fold [1/5] Epoch: 76, Val Acc: 0.7222\n",
            "Fold [1/5] Epoch: 77, Val Acc: 0.7559\n",
            "Fold [1/5] Epoch: 78, Val Acc: 0.7189\n",
            "Fold [1/5] Epoch: 79, Val Acc: 0.7424\n",
            "Fold [1/5] Epoch: 80, Val Acc: 0.7576\n",
            "Fold [1/5] Epoch: 81, Val Acc: 0.7694\n",
            "Fold [1/5] Epoch: 82, Val Acc: 0.7391\n",
            "Fold [1/5] Epoch: 83, Val Acc: 0.7121\n",
            "Fold [1/5] Epoch: 84, Val Acc: 0.7542\n",
            "Fold [1/5] Epoch: 85, Val Acc: 0.7525\n",
            "Fold [1/5] Epoch: 86, Val Acc: 0.7374\n",
            "Fold [1/5] Epoch: 87, Val Acc: 0.7256\n",
            "Fold [1/5] Epoch: 88, Val Acc: 0.7071\n",
            "Fold [1/5] Epoch: 89, Val Acc: 0.7391\n",
            "Fold [1/5] Epoch: 90, Val Acc: 0.7542\n",
            "Fold [1/5] Epoch: 91, Val Acc: 0.7475\n",
            "Fold [1/5] Epoch: 92, Val Acc: 0.7525\n",
            "Fold [1/5] Epoch: 93, Val Acc: 0.7407\n",
            "Fold [1/5] Epoch: 94, Val Acc: 0.7492\n",
            "Fold [1/5] Epoch: 95, Val Acc: 0.7609\n",
            "Fold [1/5] Epoch: 96, Val Acc: 0.7407\n",
            "Fold [1/5] Epoch: 97, Val Acc: 0.7458\n",
            "Fold [1/5] Epoch: 98, Val Acc: 0.7626\n",
            "Fold [1/5] Epoch: 99, Val Acc: 0.7407\n",
            "Fold [1/5] Epoch: 100, Val Acc: 0.7643\n",
            "Final Test Accuracy: 0.6731\n",
            "Fold [2/5]\n",
            "Fold [2/5] Epoch: 1, Val Acc: 0.6304\n",
            "Fold [2/5] Epoch: 2, Val Acc: 0.6656\n",
            "Fold [2/5] Epoch: 3, Val Acc: 0.6388\n",
            "Fold [2/5] Epoch: 4, Val Acc: 0.6355\n",
            "Fold [2/5] Epoch: 5, Val Acc: 0.6622\n",
            "Fold [2/5] Epoch: 6, Val Acc: 0.6003\n",
            "Fold [2/5] Epoch: 7, Val Acc: 0.5652\n",
            "Fold [2/5] Epoch: 8, Val Acc: 0.6472\n",
            "Fold [2/5] Epoch: 9, Val Acc: 0.6572\n",
            "Fold [2/5] Epoch: 10, Val Acc: 0.6923\n",
            "Fold [2/5] Epoch: 11, Val Acc: 0.6522\n",
            "Fold [2/5] Epoch: 12, Val Acc: 0.6187\n",
            "Fold [2/5] Epoch: 13, Val Acc: 0.6706\n",
            "Fold [2/5] Epoch: 14, Val Acc: 0.6839\n",
            "Fold [2/5] Epoch: 15, Val Acc: 0.6990\n",
            "Fold [2/5] Epoch: 16, Val Acc: 0.6873\n",
            "Fold [2/5] Epoch: 17, Val Acc: 0.6538\n",
            "Fold [2/5] Epoch: 18, Val Acc: 0.7023\n",
            "Fold [2/5] Epoch: 19, Val Acc: 0.7140\n",
            "Fold [2/5] Epoch: 20, Val Acc: 0.6923\n",
            "Fold [2/5] Epoch: 21, Val Acc: 0.6856\n",
            "Fold [2/5] Epoch: 22, Val Acc: 0.7174\n",
            "Fold [2/5] Epoch: 23, Val Acc: 0.7107\n",
            "Fold [2/5] Epoch: 24, Val Acc: 0.7107\n",
            "Fold [2/5] Epoch: 25, Val Acc: 0.7241\n",
            "Fold [2/5] Epoch: 26, Val Acc: 0.6906\n",
            "Fold [2/5] Epoch: 27, Val Acc: 0.7207\n",
            "Fold [2/5] Epoch: 28, Val Acc: 0.7258\n",
            "Fold [2/5] Epoch: 29, Val Acc: 0.7291\n",
            "Fold [2/5] Epoch: 30, Val Acc: 0.6773\n",
            "Fold [2/5] Epoch: 31, Val Acc: 0.7191\n",
            "Fold [2/5] Epoch: 32, Val Acc: 0.7140\n",
            "Fold [2/5] Epoch: 33, Val Acc: 0.7308\n",
            "Fold [2/5] Epoch: 34, Val Acc: 0.7241\n",
            "Fold [2/5] Epoch: 35, Val Acc: 0.7090\n",
            "Fold [2/5] Epoch: 36, Val Acc: 0.6823\n",
            "Fold [2/5] Epoch: 37, Val Acc: 0.7023\n",
            "Fold [2/5] Epoch: 38, Val Acc: 0.7425\n",
            "Fold [2/5] Epoch: 39, Val Acc: 0.7207\n",
            "Fold [2/5] Epoch: 40, Val Acc: 0.7391\n",
            "Fold [2/5] Epoch: 41, Val Acc: 0.7157\n",
            "Fold [2/5] Epoch: 42, Val Acc: 0.7408\n",
            "Fold [2/5] Epoch: 43, Val Acc: 0.7291\n",
            "Fold [2/5] Epoch: 44, Val Acc: 0.7358\n",
            "Fold [2/5] Epoch: 45, Val Acc: 0.7090\n",
            "Fold [2/5] Epoch: 46, Val Acc: 0.7107\n",
            "Fold [2/5] Epoch: 47, Val Acc: 0.7174\n",
            "Fold [2/5] Epoch: 48, Val Acc: 0.7157\n",
            "Fold [2/5] Epoch: 49, Val Acc: 0.7291\n",
            "Fold [2/5] Epoch: 50, Val Acc: 0.7191\n",
            "Fold [2/5] Epoch: 51, Val Acc: 0.7207\n",
            "Fold [2/5] Epoch: 52, Val Acc: 0.7508\n",
            "Fold [2/5] Epoch: 53, Val Acc: 0.7258\n",
            "Fold [2/5] Epoch: 54, Val Acc: 0.7358\n",
            "Fold [2/5] Epoch: 55, Val Acc: 0.7074\n",
            "Fold [2/5] Epoch: 56, Val Acc: 0.7324\n",
            "Fold [2/5] Epoch: 57, Val Acc: 0.7023\n",
            "Fold [2/5] Epoch: 58, Val Acc: 0.7274\n",
            "Fold [2/5] Epoch: 59, Val Acc: 0.7475\n",
            "Fold [2/5] Epoch: 60, Val Acc: 0.7308\n",
            "Fold [2/5] Epoch: 61, Val Acc: 0.7425\n",
            "Fold [2/5] Epoch: 62, Val Acc: 0.6923\n",
            "Fold [2/5] Epoch: 63, Val Acc: 0.7375\n",
            "Fold [2/5] Epoch: 64, Val Acc: 0.7074\n",
            "Fold [2/5] Epoch: 65, Val Acc: 0.7458\n",
            "Fold [2/5] Epoch: 66, Val Acc: 0.7542\n",
            "Fold [2/5] Epoch: 67, Val Acc: 0.7207\n",
            "Fold [2/5] Epoch: 68, Val Acc: 0.7241\n",
            "Fold [2/5] Epoch: 69, Val Acc: 0.7308\n",
            "Fold [2/5] Epoch: 70, Val Acc: 0.7207\n",
            "Fold [2/5] Epoch: 71, Val Acc: 0.7191\n",
            "Fold [2/5] Epoch: 72, Val Acc: 0.7425\n",
            "Fold [2/5] Epoch: 73, Val Acc: 0.7358\n",
            "Fold [2/5] Epoch: 74, Val Acc: 0.7391\n",
            "Fold [2/5] Epoch: 75, Val Acc: 0.7191\n",
            "Fold [2/5] Epoch: 76, Val Acc: 0.7241\n",
            "Fold [2/5] Epoch: 77, Val Acc: 0.7040\n",
            "Fold [2/5] Epoch: 78, Val Acc: 0.7291\n",
            "Fold [2/5] Epoch: 79, Val Acc: 0.7441\n",
            "Fold [2/5] Epoch: 80, Val Acc: 0.7391\n",
            "Fold [2/5] Epoch: 81, Val Acc: 0.7107\n",
            "Fold [2/5] Epoch: 82, Val Acc: 0.7642\n",
            "Fold [2/5] Epoch: 83, Val Acc: 0.7258\n",
            "Fold [2/5] Epoch: 84, Val Acc: 0.7358\n",
            "Fold [2/5] Epoch: 85, Val Acc: 0.7324\n",
            "Fold [2/5] Epoch: 86, Val Acc: 0.7324\n",
            "Fold [2/5] Epoch: 87, Val Acc: 0.7358\n",
            "Fold [2/5] Epoch: 88, Val Acc: 0.7358\n",
            "Fold [2/5] Epoch: 89, Val Acc: 0.7324\n",
            "Fold [2/5] Epoch: 90, Val Acc: 0.7358\n",
            "Fold [2/5] Epoch: 91, Val Acc: 0.7258\n",
            "Fold [2/5] Epoch: 92, Val Acc: 0.7358\n",
            "Fold [2/5] Epoch: 93, Val Acc: 0.7441\n",
            "Fold [2/5] Epoch: 94, Val Acc: 0.7408\n",
            "Fold [2/5] Epoch: 95, Val Acc: 0.7408\n",
            "Fold [2/5] Epoch: 96, Val Acc: 0.7508\n",
            "Fold [2/5] Epoch: 97, Val Acc: 0.7525\n",
            "Fold [2/5] Epoch: 98, Val Acc: 0.7274\n",
            "Fold [2/5] Epoch: 99, Val Acc: 0.7391\n",
            "Fold [2/5] Epoch: 100, Val Acc: 0.7425\n",
            "Final Test Accuracy: 0.6462\n",
            "Fold [3/5]\n",
            "Fold [3/5] Epoch: 1, Val Acc: 0.6402\n",
            "Fold [3/5] Epoch: 2, Val Acc: 0.6151\n",
            "Fold [3/5] Epoch: 3, Val Acc: 0.6360\n",
            "Fold [3/5] Epoch: 4, Val Acc: 0.6444\n",
            "Fold [3/5] Epoch: 5, Val Acc: 0.6548\n",
            "Fold [3/5] Epoch: 6, Val Acc: 0.6715\n",
            "Fold [3/5] Epoch: 7, Val Acc: 0.6506\n",
            "Fold [3/5] Epoch: 8, Val Acc: 0.6360\n",
            "Fold [3/5] Epoch: 9, Val Acc: 0.6674\n",
            "Fold [3/5] Epoch: 10, Val Acc: 0.6820\n",
            "Fold [3/5] Epoch: 11, Val Acc: 0.6967\n",
            "Fold [3/5] Epoch: 12, Val Acc: 0.6736\n",
            "Fold [3/5] Epoch: 13, Val Acc: 0.6318\n",
            "Fold [3/5] Epoch: 14, Val Acc: 0.6444\n",
            "Fold [3/5] Epoch: 15, Val Acc: 0.6402\n",
            "Fold [3/5] Epoch: 16, Val Acc: 0.6736\n",
            "Fold [3/5] Epoch: 17, Val Acc: 0.6904\n",
            "Fold [3/5] Epoch: 18, Val Acc: 0.6548\n",
            "Fold [3/5] Epoch: 19, Val Acc: 0.7113\n",
            "Fold [3/5] Epoch: 20, Val Acc: 0.6841\n",
            "Fold [3/5] Epoch: 21, Val Acc: 0.7029\n",
            "Fold [3/5] Epoch: 22, Val Acc: 0.7176\n",
            "Fold [3/5] Epoch: 23, Val Acc: 0.6904\n",
            "Fold [3/5] Epoch: 24, Val Acc: 0.6925\n",
            "Fold [3/5] Epoch: 25, Val Acc: 0.6967\n",
            "Fold [3/5] Epoch: 26, Val Acc: 0.6569\n",
            "Fold [3/5] Epoch: 27, Val Acc: 0.7322\n",
            "Fold [3/5] Epoch: 28, Val Acc: 0.7029\n",
            "Fold [3/5] Epoch: 29, Val Acc: 0.6904\n",
            "Fold [3/5] Epoch: 30, Val Acc: 0.7134\n",
            "Fold [3/5] Epoch: 31, Val Acc: 0.7134\n",
            "Fold [3/5] Epoch: 32, Val Acc: 0.6192\n",
            "Fold [3/5] Epoch: 33, Val Acc: 0.7406\n",
            "Fold [3/5] Epoch: 34, Val Acc: 0.7134\n",
            "Fold [3/5] Epoch: 35, Val Acc: 0.7197\n",
            "Fold [3/5] Epoch: 36, Val Acc: 0.7176\n",
            "Fold [3/5] Epoch: 37, Val Acc: 0.6862\n",
            "Fold [3/5] Epoch: 38, Val Acc: 0.7280\n",
            "Fold [3/5] Epoch: 39, Val Acc: 0.7448\n",
            "Fold [3/5] Epoch: 40, Val Acc: 0.7448\n",
            "Fold [3/5] Epoch: 41, Val Acc: 0.7385\n",
            "Fold [3/5] Epoch: 42, Val Acc: 0.7510\n",
            "Fold [3/5] Epoch: 43, Val Acc: 0.7113\n",
            "Fold [3/5] Epoch: 44, Val Acc: 0.7238\n",
            "Fold [3/5] Epoch: 45, Val Acc: 0.7113\n",
            "Fold [3/5] Epoch: 46, Val Acc: 0.7155\n",
            "Fold [3/5] Epoch: 47, Val Acc: 0.7406\n",
            "Fold [3/5] Epoch: 48, Val Acc: 0.7427\n",
            "Fold [3/5] Epoch: 49, Val Acc: 0.7364\n",
            "Fold [3/5] Epoch: 50, Val Acc: 0.7385\n",
            "Fold [3/5] Epoch: 51, Val Acc: 0.7176\n",
            "Fold [3/5] Epoch: 52, Val Acc: 0.7573\n",
            "Fold [3/5] Epoch: 53, Val Acc: 0.7301\n",
            "Fold [3/5] Epoch: 54, Val Acc: 0.7490\n",
            "Fold [3/5] Epoch: 55, Val Acc: 0.7490\n",
            "Fold [3/5] Epoch: 56, Val Acc: 0.7322\n",
            "Fold [3/5] Epoch: 57, Val Acc: 0.7469\n",
            "Fold [3/5] Epoch: 58, Val Acc: 0.7155\n",
            "Fold [3/5] Epoch: 59, Val Acc: 0.7490\n",
            "Fold [3/5] Epoch: 60, Val Acc: 0.7259\n",
            "Fold [3/5] Epoch: 61, Val Acc: 0.7573\n",
            "Fold [3/5] Epoch: 62, Val Acc: 0.7469\n",
            "Fold [3/5] Epoch: 63, Val Acc: 0.7155\n",
            "Fold [3/5] Epoch: 64, Val Acc: 0.7343\n",
            "Fold [3/5] Epoch: 65, Val Acc: 0.7134\n",
            "Fold [3/5] Epoch: 66, Val Acc: 0.7364\n",
            "Fold [3/5] Epoch: 67, Val Acc: 0.7301\n",
            "Fold [3/5] Epoch: 68, Val Acc: 0.7343\n",
            "Fold [3/5] Epoch: 69, Val Acc: 0.7238\n",
            "Fold [3/5] Epoch: 70, Val Acc: 0.7280\n",
            "Fold [3/5] Epoch: 71, Val Acc: 0.7448\n",
            "Fold [3/5] Epoch: 72, Val Acc: 0.7469\n",
            "Fold [3/5] Epoch: 73, Val Acc: 0.7552\n",
            "Fold [3/5] Epoch: 74, Val Acc: 0.7259\n",
            "Fold [3/5] Epoch: 75, Val Acc: 0.7573\n",
            "Fold [3/5] Epoch: 76, Val Acc: 0.7657\n",
            "Fold [3/5] Epoch: 77, Val Acc: 0.7531\n",
            "Fold [3/5] Epoch: 78, Val Acc: 0.7573\n",
            "Fold [3/5] Epoch: 79, Val Acc: 0.7490\n",
            "Fold [3/5] Epoch: 80, Val Acc: 0.7510\n",
            "Fold [3/5] Epoch: 81, Val Acc: 0.7490\n",
            "Fold [3/5] Epoch: 82, Val Acc: 0.7113\n",
            "Fold [3/5] Epoch: 83, Val Acc: 0.7406\n",
            "Fold [3/5] Epoch: 84, Val Acc: 0.7594\n",
            "Fold [3/5] Epoch: 85, Val Acc: 0.6778\n",
            "Fold [3/5] Epoch: 86, Val Acc: 0.7469\n",
            "Fold [3/5] Epoch: 87, Val Acc: 0.7762\n",
            "Fold [3/5] Epoch: 88, Val Acc: 0.7741\n",
            "Fold [3/5] Epoch: 89, Val Acc: 0.7469\n",
            "Fold [3/5] Epoch: 90, Val Acc: 0.7573\n",
            "Fold [3/5] Epoch: 91, Val Acc: 0.7699\n",
            "Fold [3/5] Epoch: 92, Val Acc: 0.7490\n",
            "Fold [3/5] Epoch: 93, Val Acc: 0.7469\n",
            "Fold [3/5] Epoch: 94, Val Acc: 0.6925\n",
            "Fold [3/5] Epoch: 95, Val Acc: 0.7490\n",
            "Fold [3/5] Epoch: 96, Val Acc: 0.7531\n",
            "Fold [3/5] Epoch: 97, Val Acc: 0.7615\n",
            "Fold [3/5] Epoch: 98, Val Acc: 0.7197\n",
            "Fold [3/5] Epoch: 99, Val Acc: 0.7218\n",
            "Fold [3/5] Epoch: 100, Val Acc: 0.7699\n",
            "Final Test Accuracy: 0.6577\n",
            "Fold [4/5]\n",
            "Fold [4/5] Epoch: 1, Val Acc: 0.6650\n",
            "Fold [4/5] Epoch: 2, Val Acc: 0.6505\n",
            "Fold [4/5] Epoch: 3, Val Acc: 0.6926\n",
            "Fold [4/5] Epoch: 4, Val Acc: 0.5858\n",
            "Fold [4/5] Epoch: 5, Val Acc: 0.6278\n",
            "Fold [4/5] Epoch: 6, Val Acc: 0.6408\n",
            "Fold [4/5] Epoch: 7, Val Acc: 0.6731\n",
            "Fold [4/5] Epoch: 8, Val Acc: 0.6650\n",
            "Fold [4/5] Epoch: 9, Val Acc: 0.6553\n",
            "Fold [4/5] Epoch: 10, Val Acc: 0.6699\n",
            "Fold [4/5] Epoch: 11, Val Acc: 0.6311\n",
            "Fold [4/5] Epoch: 12, Val Acc: 0.6877\n",
            "Fold [4/5] Epoch: 13, Val Acc: 0.6780\n",
            "Fold [4/5] Epoch: 14, Val Acc: 0.6828\n",
            "Fold [4/5] Epoch: 15, Val Acc: 0.6570\n",
            "Fold [4/5] Epoch: 16, Val Acc: 0.6537\n",
            "Fold [4/5] Epoch: 17, Val Acc: 0.6893\n",
            "Fold [4/5] Epoch: 18, Val Acc: 0.6958\n",
            "Fold [4/5] Epoch: 19, Val Acc: 0.6877\n",
            "Fold [4/5] Epoch: 20, Val Acc: 0.6893\n",
            "Fold [4/5] Epoch: 21, Val Acc: 0.6634\n",
            "Fold [4/5] Epoch: 22, Val Acc: 0.6780\n",
            "Fold [4/5] Epoch: 23, Val Acc: 0.6731\n",
            "Fold [4/5] Epoch: 24, Val Acc: 0.6893\n",
            "Fold [4/5] Epoch: 25, Val Acc: 0.6214\n",
            "Fold [4/5] Epoch: 26, Val Acc: 0.6990\n",
            "Fold [4/5] Epoch: 27, Val Acc: 0.7184\n",
            "Fold [4/5] Epoch: 28, Val Acc: 0.6926\n",
            "Fold [4/5] Epoch: 29, Val Acc: 0.6909\n",
            "Fold [4/5] Epoch: 30, Val Acc: 0.6909\n",
            "Fold [4/5] Epoch: 31, Val Acc: 0.7233\n",
            "Fold [4/5] Epoch: 32, Val Acc: 0.6796\n",
            "Fold [4/5] Epoch: 33, Val Acc: 0.7087\n",
            "Fold [4/5] Epoch: 34, Val Acc: 0.6893\n",
            "Fold [4/5] Epoch: 35, Val Acc: 0.6812\n",
            "Fold [4/5] Epoch: 36, Val Acc: 0.7120\n",
            "Fold [4/5] Epoch: 37, Val Acc: 0.7168\n",
            "Fold [4/5] Epoch: 38, Val Acc: 0.7023\n",
            "Fold [4/5] Epoch: 39, Val Acc: 0.7006\n",
            "Fold [4/5] Epoch: 40, Val Acc: 0.7039\n",
            "Fold [4/5] Epoch: 41, Val Acc: 0.7136\n",
            "Fold [4/5] Epoch: 42, Val Acc: 0.7362\n",
            "Fold [4/5] Epoch: 43, Val Acc: 0.7152\n",
            "Fold [4/5] Epoch: 44, Val Acc: 0.7023\n",
            "Fold [4/5] Epoch: 45, Val Acc: 0.7168\n",
            "Fold [4/5] Epoch: 46, Val Acc: 0.6699\n",
            "Fold [4/5] Epoch: 47, Val Acc: 0.7201\n",
            "Fold [4/5] Epoch: 48, Val Acc: 0.6909\n",
            "Fold [4/5] Epoch: 49, Val Acc: 0.7039\n",
            "Fold [4/5] Epoch: 50, Val Acc: 0.7184\n",
            "Fold [4/5] Epoch: 51, Val Acc: 0.7023\n",
            "Fold [4/5] Epoch: 52, Val Acc: 0.7055\n",
            "Fold [4/5] Epoch: 53, Val Acc: 0.7120\n",
            "Fold [4/5] Epoch: 54, Val Acc: 0.6974\n",
            "Fold [4/5] Epoch: 55, Val Acc: 0.7136\n",
            "Fold [4/5] Epoch: 56, Val Acc: 0.7168\n",
            "Fold [4/5] Epoch: 57, Val Acc: 0.7120\n",
            "Fold [4/5] Epoch: 58, Val Acc: 0.6828\n",
            "Fold [4/5] Epoch: 59, Val Acc: 0.7217\n",
            "Fold [4/5] Epoch: 60, Val Acc: 0.7265\n",
            "Fold [4/5] Epoch: 61, Val Acc: 0.7379\n",
            "Fold [4/5] Epoch: 62, Val Acc: 0.7249\n",
            "Fold [4/5] Epoch: 63, Val Acc: 0.7055\n",
            "Fold [4/5] Epoch: 64, Val Acc: 0.7362\n",
            "Fold [4/5] Epoch: 65, Val Acc: 0.6990\n",
            "Fold [4/5] Epoch: 66, Val Acc: 0.7362\n",
            "Fold [4/5] Epoch: 67, Val Acc: 0.7168\n",
            "Fold [4/5] Epoch: 68, Val Acc: 0.6974\n",
            "Fold [4/5] Epoch: 69, Val Acc: 0.6812\n",
            "Fold [4/5] Epoch: 70, Val Acc: 0.6974\n",
            "Fold [4/5] Epoch: 71, Val Acc: 0.7184\n",
            "Fold [4/5] Epoch: 72, Val Acc: 0.7184\n",
            "Fold [4/5] Epoch: 73, Val Acc: 0.7184\n",
            "Fold [4/5] Epoch: 74, Val Acc: 0.7249\n",
            "Fold [4/5] Epoch: 75, Val Acc: 0.7298\n",
            "Fold [4/5] Epoch: 76, Val Acc: 0.7379\n",
            "Fold [4/5] Epoch: 77, Val Acc: 0.7201\n",
            "Fold [4/5] Epoch: 78, Val Acc: 0.7120\n",
            "Fold [4/5] Epoch: 79, Val Acc: 0.7071\n",
            "Fold [4/5] Epoch: 80, Val Acc: 0.7217\n",
            "Fold [4/5] Epoch: 81, Val Acc: 0.7233\n",
            "Fold [4/5] Epoch: 82, Val Acc: 0.7136\n",
            "Fold [4/5] Epoch: 83, Val Acc: 0.6780\n",
            "Fold [4/5] Epoch: 84, Val Acc: 0.7249\n",
            "Fold [4/5] Epoch: 85, Val Acc: 0.6748\n",
            "Fold [4/5] Epoch: 86, Val Acc: 0.6553\n",
            "Fold [4/5] Early stopping at epoch 86\n",
            "Final Test Accuracy: 0.6154\n",
            "Fold [5/5]\n",
            "Fold [5/5] Epoch: 1, Val Acc: 0.5870\n",
            "Fold [5/5] Epoch: 2, Val Acc: 0.5635\n",
            "Fold [5/5] Epoch: 3, Val Acc: 0.6338\n",
            "Fold [5/5] Epoch: 4, Val Acc: 0.6605\n",
            "Fold [5/5] Epoch: 5, Val Acc: 0.6472\n",
            "Fold [5/5] Epoch: 6, Val Acc: 0.6639\n",
            "Fold [5/5] Epoch: 7, Val Acc: 0.6271\n",
            "Fold [5/5] Epoch: 8, Val Acc: 0.6355\n",
            "Fold [5/5] Epoch: 9, Val Acc: 0.6388\n",
            "Fold [5/5] Epoch: 10, Val Acc: 0.6455\n",
            "Fold [5/5] Epoch: 11, Val Acc: 0.6722\n",
            "Fold [5/5] Epoch: 12, Val Acc: 0.6505\n",
            "Fold [5/5] Epoch: 13, Val Acc: 0.6304\n",
            "Fold [5/5] Epoch: 14, Val Acc: 0.6706\n",
            "Fold [5/5] Epoch: 15, Val Acc: 0.6756\n",
            "Fold [5/5] Epoch: 16, Val Acc: 0.6823\n",
            "Fold [5/5] Epoch: 17, Val Acc: 0.6639\n",
            "Fold [5/5] Epoch: 18, Val Acc: 0.6689\n",
            "Fold [5/5] Epoch: 19, Val Acc: 0.6488\n",
            "Fold [5/5] Epoch: 20, Val Acc: 0.6839\n",
            "Fold [5/5] Epoch: 21, Val Acc: 0.6572\n",
            "Fold [5/5] Epoch: 22, Val Acc: 0.6555\n",
            "Fold [5/5] Epoch: 23, Val Acc: 0.6622\n",
            "Fold [5/5] Epoch: 24, Val Acc: 0.6856\n",
            "Fold [5/5] Epoch: 25, Val Acc: 0.6505\n",
            "Fold [5/5] Epoch: 26, Val Acc: 0.6940\n",
            "Fold [5/5] Epoch: 27, Val Acc: 0.6873\n",
            "Fold [5/5] Epoch: 28, Val Acc: 0.7124\n",
            "Fold [5/5] Epoch: 29, Val Acc: 0.6957\n",
            "Fold [5/5] Epoch: 30, Val Acc: 0.7023\n",
            "Fold [5/5] Epoch: 31, Val Acc: 0.7040\n",
            "Fold [5/5] Epoch: 32, Val Acc: 0.7023\n",
            "Fold [5/5] Epoch: 33, Val Acc: 0.6672\n",
            "Fold [5/5] Epoch: 34, Val Acc: 0.7124\n",
            "Fold [5/5] Epoch: 35, Val Acc: 0.6890\n",
            "Fold [5/5] Epoch: 36, Val Acc: 0.7007\n",
            "Fold [5/5] Epoch: 37, Val Acc: 0.6739\n",
            "Fold [5/5] Epoch: 38, Val Acc: 0.6973\n",
            "Fold [5/5] Epoch: 39, Val Acc: 0.6839\n",
            "Fold [5/5] Epoch: 40, Val Acc: 0.7107\n",
            "Fold [5/5] Epoch: 41, Val Acc: 0.7057\n",
            "Fold [5/5] Epoch: 42, Val Acc: 0.7375\n",
            "Fold [5/5] Epoch: 43, Val Acc: 0.7074\n",
            "Fold [5/5] Epoch: 44, Val Acc: 0.7191\n",
            "Fold [5/5] Epoch: 45, Val Acc: 0.7191\n",
            "Fold [5/5] Epoch: 46, Val Acc: 0.7458\n",
            "Fold [5/5] Epoch: 47, Val Acc: 0.7358\n",
            "Fold [5/5] Epoch: 48, Val Acc: 0.7391\n",
            "Fold [5/5] Epoch: 49, Val Acc: 0.7525\n",
            "Fold [5/5] Epoch: 50, Val Acc: 0.7291\n",
            "Fold [5/5] Epoch: 51, Val Acc: 0.7241\n",
            "Fold [5/5] Epoch: 52, Val Acc: 0.7107\n",
            "Fold [5/5] Epoch: 53, Val Acc: 0.6555\n",
            "Fold [5/5] Epoch: 54, Val Acc: 0.7324\n",
            "Fold [5/5] Epoch: 55, Val Acc: 0.7207\n",
            "Fold [5/5] Epoch: 56, Val Acc: 0.7224\n",
            "Fold [5/5] Epoch: 57, Val Acc: 0.7140\n",
            "Fold [5/5] Epoch: 58, Val Acc: 0.7090\n",
            "Fold [5/5] Epoch: 59, Val Acc: 0.7124\n",
            "Fold [5/5] Epoch: 60, Val Acc: 0.7140\n",
            "Fold [5/5] Epoch: 61, Val Acc: 0.7090\n",
            "Fold [5/5] Epoch: 62, Val Acc: 0.7308\n",
            "Fold [5/5] Epoch: 63, Val Acc: 0.7508\n",
            "Fold [5/5] Epoch: 64, Val Acc: 0.7408\n",
            "Fold [5/5] Epoch: 65, Val Acc: 0.7074\n",
            "Fold [5/5] Epoch: 66, Val Acc: 0.7458\n",
            "Fold [5/5] Epoch: 67, Val Acc: 0.7291\n",
            "Fold [5/5] Epoch: 68, Val Acc: 0.7157\n",
            "Fold [5/5] Epoch: 69, Val Acc: 0.7274\n",
            "Fold [5/5] Epoch: 70, Val Acc: 0.7258\n",
            "Fold [5/5] Epoch: 71, Val Acc: 0.7140\n",
            "Fold [5/5] Epoch: 72, Val Acc: 0.7274\n",
            "Fold [5/5] Epoch: 73, Val Acc: 0.7241\n",
            "Fold [5/5] Epoch: 74, Val Acc: 0.7458\n",
            "Fold [5/5] Early stopping at epoch 74\n",
            "Final Test Accuracy: 0.6500\n",
            "True Positives (TP): 97\n",
            "False Positives (FP): 58\n",
            "True Negatives (TN): 72\n",
            "False Negatives (FN): 33\n",
            "Accuracy: 0.65\n",
            "Precision: 0.6258064516129033\n",
            "Recall: 0.7461538461538462\n",
            "F1-Score: 0.6807017543859649\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GCNConv, global_max_pool\n",
        "from torch.nn import Linear\n",
        "from ogb.graphproppred import PygGraphPropPredDataset\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Define a simple Graph Neural Network\n",
        "# Definisce una nuova classe chiamata GNN, che eredita dalla classe torch.nn.Module\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GNN, self).__init__()\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv4 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin = Linear(hidden_channels, hidden_channels)\n",
        "        self.lin1 = Linear(hidden_channels, 2) # Output will be 2 classes\n",
        "\n",
        "\n",
        "# Questo metodo definisce come i dati vengono passati attraverso la rete neurale durante la fase di \"forward pass\". Durante la fase di forward\n",
        "# pass, i dati di input vengono elaborati attraverso i vari strati della rete fino a produrre l'output finale.\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv4(x, edge_index)\n",
        "        x = x.relu()\n",
        "        #x = global_max_pool(x, batch)  # Global pooling\n",
        "        x = self.lin(x)\n",
        "        #x = x.relu()\n",
        "        x = global_max_pool(x, batch)  # Global pooling\n",
        "        x = self.lin1(x)\n",
        "        return x\n",
        "\n",
        "# Define train and validate functions\n",
        "# è una funzione che gestisce il processo di addestramento di una rete neurale\n",
        "def train(model, optimizer, train_loader):\n",
        "    model.train()\n",
        "    for data in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x.float(), data.edge_index, data.batch)\n",
        "        loss = F.cross_entropy(out, data.y.view(-1).long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "# è utilizzata per valutare le prestazioni di un modello di rete neurale su un dataset di validation.\n",
        "def validate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            pred = model(data.x.float(), data.edge_index, data.batch)\n",
        "            pred = pred.argmax(dim=1)\n",
        "            correct += int((pred == data.y.view(-1)).sum())\n",
        "            total += data.y.size(0)\n",
        "            all_predictions.extend(pred.tolist())\n",
        "            all_labels.extend(data.y.view(-1).tolist())\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy, all_predictions, all_labels\n",
        "\n",
        "# Load dataset\n",
        "#utilizza un dataset chiamato ogbg-molhiv dal modulo ogb.graphproppred.PygGraphPropPredDataset per il problema di previsione delle proprietà molecolari.\n",
        "dataset = PygGraphPropPredDataset(name='ogbg-molhiv')\n",
        "split_idx = dataset.get_idx_split()\n",
        "train_idx, val_idx, test_idx=split_idx['train'], split_idx['valid'], split_idx['test']\n",
        "train_data = dataset[train_idx]\n",
        "val_data = dataset[val_idx]\n",
        "test_data = dataset[test_idx]\n",
        "\n",
        "# Define cross-validation settings\n",
        "# divide i risultati della cross in folds (5) ed effettua un'altra cross tra le folds\n",
        "k_folds = 5\n",
        "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "label_0_indices_te = (test_data.y == 0).nonzero(as_tuple=True)[0].reshape(-1)\n",
        "label_1_indices_te = (test_data.y == 1).nonzero(as_tuple=True)[0].reshape(-1)\n",
        "\n",
        "min_size_test = min(label_0_indices_te.size(0), label_1_indices_te.size(0))\n",
        "\n",
        "# Shuffle indices randomly\n",
        "label_0_indices_te = label_0_indices_te[torch.randperm(label_0_indices_te.size(0))]\n",
        "label_1_indices_te = label_1_indices_te[torch.randperm(label_1_indices_te.size(0))]\n",
        "\n",
        "label_0_indices_te = label_0_indices_te[:min_size_test]\n",
        "label_1_indices_te = label_1_indices_te[:min_size_test]\n",
        "\n",
        "balanced_indices_te = torch.cat([label_0_indices_te, label_1_indices_te])\n",
        "\n",
        "balanced_test_data = test_data[balanced_indices_te]\n",
        "\n",
        "test_loader = DataLoader(balanced_test_data, batch_size=32, shuffle=False)\n",
        "\n",
        "# Training loop with cross-validation\n",
        "# implementa un loop di addestramento e validazione utilizzando la tecnica della cross-validation a k-fold.\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
        "    print(f\"Fold [{fold+1}/{k_folds}]\")\n",
        "\n",
        "    # Subset the data for this fold\n",
        "    train_data = dataset[train_idx]\n",
        "    val_data = dataset[val_idx]\n",
        "\n",
        "    # Separate data by class labels\n",
        "    label_0_indices_tr = (train_data.y == 0).nonzero(as_tuple=True)[0].reshape(-1)\n",
        "    label_1_indices_tr = (train_data.y == 1).nonzero(as_tuple=True)[0].reshape(-1)\n",
        "\n",
        "    label_0_indices_vl = (val_data.y == 0).nonzero(as_tuple=True)[0].reshape(-1)\n",
        "    label_1_indices_vl = (val_data.y == 1).nonzero(as_tuple=True)[0].reshape(-1)\n",
        "\n",
        "    # Determine the size of the smaller class\n",
        "    min_size_train = min(label_0_indices_tr.size(0), label_1_indices_tr.size(0))\n",
        "    min_size_val = min(label_0_indices_vl.size(0), label_1_indices_vl.size(0))\n",
        "\n",
        "    # Sample equal number of elements from each class\n",
        "    label_0_indices_tr = label_0_indices_tr[:min_size_train]\n",
        "    label_1_indices_tr = label_1_indices_tr[:min_size_train]\n",
        "\n",
        "    label_0_indices_vl = label_0_indices_vl[:min_size_val]\n",
        "    label_1_indices_vl = label_1_indices_vl[:min_size_val]\n",
        "\n",
        "    # Concatenate the indices\n",
        "    balanced_indices_tr = torch.cat([label_0_indices_tr, label_1_indices_tr])\n",
        "    balanced_indices_vl = torch.cat([label_0_indices_vl, label_1_indices_vl])\n",
        "\n",
        "    # Create a new dataset with balanced samples\n",
        "    balanced_train_data = train_data[balanced_indices_tr]\n",
        "    balanced_val_data = val_data[balanced_indices_vl]\n",
        "\n",
        "    train_loader = DataLoader(balanced_train_data, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(balanced_val_data, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Define the model and optimizer\n",
        "    model = GNN(hidden_channels=64)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    # Early stopping variables\n",
        "    best_val_acc = 0.0\n",
        "    patience = 25  # Stop after no improvement for 25 epochs\n",
        "    counter = 0\n",
        "\n",
        "    # Training loop with validation and early stopping\n",
        "    # rappresenta il ciclo di addestramento e validazione del modello all'interno di un determinato fold durante il processo di cross-validation\n",
        "    for epoch in range(1, 101):  # Train for maximum 100 epochs\n",
        "        #Train the model\n",
        "        train(model, optimizer, train_loader)\n",
        "\n",
        "        # Validate the model\n",
        "        val_acc, _, _ = validate(model, val_loader)\n",
        "\n",
        "        print(f'Fold [{fold+1}/{k_folds}] Epoch: {epoch}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "        # Check for improvement in validation accuracy\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            counter = 0  # Reset counter\n",
        "        else:\n",
        "            counter += 1  # No improvement, increase counter\n",
        "\n",
        "        # If no improvement for \"patience\" number of epochs, stop training\n",
        "        if counter >= patience:\n",
        "            print(f'Fold [{fold+1}/{k_folds}] Early stopping at epoch {epoch}')\n",
        "            break\n",
        "\n",
        "    # Final evaluation on the test set\n",
        "    # per valutare le prestazioni del modello sul set di dati di test e stampare l'accuratezza finale del test.\n",
        "    test_acc, all_test_predictions, all_test_labels = validate(model, test_loader)\n",
        "    print(f'Final Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "\n",
        "# Convert predictions and labels to numpy arrays\n",
        "all_test_predictions = np.array(all_test_predictions)\n",
        "all_test_labels = np.array(all_test_labels)\n",
        "\n",
        "# Calculate True Positives (TP), False Positives (FP),\n",
        "# True Negatives (TN), False Negatives (FN)\n",
        "TP = np.sum((all_test_predictions == 1) & (all_test_labels == 1))\n",
        "FP = np.sum((all_test_predictions == 1) & (all_test_labels == 0))\n",
        "TN = np.sum((all_test_predictions == 0) & (all_test_labels == 0))\n",
        "FN = np.sum((all_test_predictions == 0) & (all_test_labels == 1))\n",
        "\n",
        "print(\"True Positives (TP):\", TP)\n",
        "print(\"False Positives (FP):\", FP)\n",
        "print(\"True Negatives (TN):\", TN)\n",
        "print(\"False Negatives (FN):\", FN)\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1-score\n",
        "accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
        "precision = TP / (TP + FP)\n",
        "recall = TP / (TP + FN)\n",
        "f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBg0s7WmXVv8",
        "outputId": "90183554-9199-455a-cd10-875eaecf0529"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Grafo interpolato 1:\n",
            "Data(edge_index=[2, 128])\n",
            "Grafo interpolato 2:\n",
            "Data(edge_index=[2, 128])\n",
            "Grafo interpolato 3:\n",
            "Data(edge_index=[2, 128])\n",
            "Grafo interpolato 4:\n",
            "Data(edge_index=[2, 128])\n",
            "Grafo interpolato 5:\n",
            "Data(edge_index=[2, 128])\n",
            "Grafo interpolato 6:\n",
            "Data(edge_index=[2, 128])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# Funzione per generare grafi intermedi tra due grafi senza considerare i pesi sugli archi\n",
        "# prende in input due grafi e genera una serie di grafi interpolati tra di essi.\n",
        "def interpolate_graphs(graph1, graph2, num_steps=5):\n",
        "    interpolated_graphs = []\n",
        "    for i in range(num_steps + 1):\n",
        "        # Calcola i pesi per l'interpolazione\n",
        "        weight1 = i / num_steps\n",
        "        weight2 = 1 - weight1\n",
        "\n",
        "        # Interpola gli indici degli archi\n",
        "        interpolated_edge_index = torch.cat([graph1.edge_index, graph2.edge_index], dim=-1)\n",
        "\n",
        "        # Crea un nuovo grafo interpolato\n",
        "        interpolated_data = Data(edge_index=interpolated_edge_index)\n",
        "\n",
        "        # Aggiunge il grafo interpolato alla lista\n",
        "        interpolated_graphs.append(interpolated_data)\n",
        "\n",
        "    return interpolated_graphs\n",
        "\n",
        "# Esempio di utilizzo\n",
        "# Supponiamo di avere due grafi esistenti graph1 e graph2\n",
        "graph1 = train_data[0]\n",
        "graph2 = train_data[1]\n",
        "\n",
        "# Genera 5 grafi intermedi tra graph1 e graph2\n",
        "interpolated_graphs = interpolate_graphs(graph1, graph2, num_steps=5)\n",
        "\n",
        "# Stampiamo i grafi intermedi generati\n",
        "for i, graph_data in enumerate(interpolated_graphs):\n",
        "    print(f\"Grafo interpolato {i+1}:\")\n",
        "    print(graph_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "id": "YwcgWJhecMUy",
        "outputId": "afd94a7a-5ea4-447a-9585-dd06ee76e875"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'GlobalStorage' object has no attribute 'edges'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-79409762d349>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'skyblue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfont_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfont_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bold'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Grafo Originale\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/networkx/drawing/nx_pylab.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(G, pos, ax, **kwds)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"with_labels\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"labels\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mdraw_networkx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis_off\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_if_interactive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/networkx/drawing/nx_pylab.py\u001b[0m in \u001b[0;36mdraw_networkx\u001b[0;34m(G, pos, arrows, with_labels, **kwds)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0mdraw_networkx_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnode_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m     \u001b[0mdraw_networkx_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0medge_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwith_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mdraw_networkx_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mlabel_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/networkx/drawing/nx_pylab.py\u001b[0m in \u001b[0;36mdraw_networkx_edges\u001b[0;34m(G, pos, edgelist, width, edge_color, style, alpha, arrowstyle, arrowsize, edge_cmap, edge_vmin, edge_vmax, ax, arrows, label, node_size, nodelist, node_shape, connectionstyle, min_source_margin, min_target_margin)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0medgelist\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m         \u001b[0medgelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medgelist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# no edges!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/data/data.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    557\u001b[0m                 \u001b[0;34m\"dataset, remove the 'processed/' directory in the dataset's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m                 \"root folder and try again.\")\n\u001b[0;32m--> 559\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_store\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/data/storage.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0;34mf\"'{self.__class__.__name__}' object has no attribute '{key}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             ) from None\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GlobalStorage' object has no attribute 'edges'"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "# Funzione per aggiungere rumore ai valori degli attributi dei nodi\n",
        "def add_noise_to_node_attributes(graph, noise_level=0.1):\n",
        "    noisy_graph = nx.Graph()\n",
        "\n",
        "    # Aggiungi i nodi al grafo\n",
        "    for i in range(graph.num_nodes):\n",
        "        noisy_graph.add_node(i)\n",
        "\n",
        "    # Aggiungi gli archi al grafo\n",
        "    for j in range(graph.num_edges):\n",
        "        src, dst = graph.edge_index[:, j]\n",
        "        noisy_graph.add_edge(src.item(), dst.item())\n",
        "\n",
        "    # Aggiungi rumore agli attributi dei nodi\n",
        "    for node_index in range(graph.num_nodes):\n",
        "        node_attributes = graph.x[node_index].numpy()\n",
        "        noisy_attributes = node_attributes + np.random.normal(0, noise_level, size=node_attributes.shape)\n",
        "        noisy_graph.nodes[node_index]['attributes'] = noisy_attributes\n",
        "\n",
        "    return noisy_graph\n",
        "\n",
        "# Applica l'aumentazione agli attributi del primo grafo nel dataset di addestramento\n",
        "original_graph_data = train_data[0]\n",
        "noisy_graph = add_noise_to_node_attributes(original_graph_data, noise_level=0.1)\n",
        "\n",
        "# Visualizza il grafo originale e il grafo con attributi rumorosi\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "nx.draw(train_data[0], with_labels=True, node_color='skyblue', node_size=800, font_size=12, font_weight='bold')\n",
        "plt.title(\"Grafo Originale\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "nx.draw(noisy_graph, with_labels=True, node_color='salmon', node_size=800, font_size=12, font_weight='bold')\n",
        "plt.title(\"Grafo con Attributi Rumorosi\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "I2uaQuEzPrjh",
        "outputId": "33f52280-8d7c-4dc4-ff13-7faea434666f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting deepchem\n",
            "  Downloading deepchem-2.7.1-py3-none-any.whl (693 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m693.2/693.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.2.2)\n",
            "Collecting scipy<1.9 (from deepchem)\n",
            "  Downloading scipy-1.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rdkit in /usr/local/lib/python3.10/dist-packages (from deepchem) (2023.9.5)\n",
            "Collecting numpy>=1.21 (from deepchem)\n",
            "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2023.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit->deepchem) (9.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->deepchem) (3.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->deepchem) (1.16.0)\n",
            "Installing collected packages: numpy, scipy, deepchem\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.4\n",
            "    Uninstalling scipy-1.11.4:\n",
            "      Successfully uninstalled scipy-1.11.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jax 0.4.23 requires scipy>=1.9, but you have scipy 1.8.1 which is incompatible.\n",
            "jaxlib 0.4.23+cuda12.cudnn89 requires scipy>=1.9, but you have scipy 1.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed deepchem-2.7.1 numpy-1.24.4 scipy-1.8.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "58b300eb0aa04e3488a886041f0f1342",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/deprecation.py:588: calling function (from tensorflow.python.eager.polymorphic_function.polymorphic_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "experimental_relax_shapes is deprecated, use reduce_retracing instead\n",
            "WARNING:deepchem.models:Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'pytorch_lightning'\n",
            "WARNING:deepchem.models:Skipped loading some Jax models, missing a dependency. No module named 'haiku'\n"
          ]
        }
      ],
      "source": [
        "# installazione e all'importazione di DeepChem, una libreria open-source progettata per l'apprendimento automatico in chimica\n",
        "!pip install deepchem\n",
        "import deepchem as dc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6CUNEBxTE-2"
      },
      "source": [
        "NON ABBIA IDs quindi non va"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "kr293lUNPz0B",
        "outputId": "fb5acc09-3299-49ec-f7fb-fef1a56c793b"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'PygGraphPropPredDataset' object has no attribute 'ids'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a88a7395a57a>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPygGraphPropPredDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ogbg-molhiv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mscaffoldsplitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScaffoldSplitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaffoldsplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaffold_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaffold_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaffold_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deepchem/splits/splitters.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, dataset, frac_train, frac_valid, frac_test, seed, log_every_n)\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \"\"\"\n\u001b[1;32m   1410\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_almost_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac_train\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfrac_valid\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfrac_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1411\u001b[0;31m     \u001b[0mscaffold_sets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_scaffolds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m     \u001b[0mtrain_cutoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrac_train\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deepchem/splits/splitters.py\u001b[0m in \u001b[0;36mgenerate_scaffolds\u001b[0;34m(self, dataset, log_every_n)\u001b[0m\n\u001b[1;32m   1450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"About to generate scaffolds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1452\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmiles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1453\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mind\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_every_n\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1454\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generating scaffold %d/%d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_data_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         raise AttributeError(f\"'{self.__class__.__name__}' object has no \"\n\u001b[0m\u001b[1;32m    319\u001b[0m                              f\"attribute '{key}'\")\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PygGraphPropPredDataset' object has no attribute 'ids'"
          ]
        }
      ],
      "source": [
        "# suddividere i dati in set di addestramento, validation e test utilizzando la tecnica di suddivisione per scaffold.\n",
        "import deepchem as dc\n",
        "from ogb.graphproppred import PygGraphPropPredDataset\n",
        "dataset = PygGraphPropPredDataset(name='ogbg-molhiv')\n",
        "scaffoldsplitter = dc.splits.ScaffoldSplitter()\n",
        "train,test = scaffoldsplitter.split(dataset)\n",
        "train_idx, valid_idx, test_idx = scaffold_split['train'], scaffold_split['valid'], scaffold_split['test']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "B9WFzwj9O1SF",
        "outputId": "751d89ad-99bd-46f7-b023-fa2f3ef32baa"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'balanced_test_data' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-bfd864998c15>\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbalanced_test_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Training loop with cross-validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'balanced_test_data' is not defined"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GCNConv, global_max_pool\n",
        "from torch.nn import Linear\n",
        "from ogb.graphproppred import PygGraphPropPredDataset\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Define a simple Graph Neural Network\n",
        "# definisce una rete neurale per grafi (GNN) usando il modulo torch\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GNN, self).__init__()\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv4 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin = Linear(hidden_channels, 2)  # Output will be 2 classes\n",
        "\n",
        "\n",
        "# è la funzione forward definita all'interno della classe GNN\n",
        "# viene utilizzata per definire il flusso in avanti (forward pass) del modello.\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv4(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = global_max_pool(x, batch)  # Global pooling\n",
        "        x = self.lin(x)\n",
        "        return x\n",
        "\n",
        "# Define train and validate functions\n",
        "# gestisce il processo di addestramento di un modello di apprendimento automatico.\n",
        "def train(model, optimizer, train_loader):\n",
        "    model.train()\n",
        "    for data in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x.float(), data.edge_index, data.batch)\n",
        "        loss = F.cross_entropy(out, data.y.view(-1).long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# utilizzata per valutare le prestazioni di un modello di apprendimento automatico su un insieme di dati di validazione o test.\n",
        "def validate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            pred = model(data.x.float(), data.edge_index, data.batch)\n",
        "            pred = pred.argmax(dim=1)\n",
        "            correct += int((pred == data.y.view(-1)).sum())\n",
        "            total += data.y.size(0)\n",
        "            all_predictions.extend(pred.tolist())\n",
        "            all_labels.extend(data.y.view(-1).tolist())\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy, all_predictions, all_labels\n",
        "\n",
        "# Load dataset\n",
        "# preparazione dei dati e la configurazione del processo di addestramento del modello.\n",
        "dataset = PygGraphPropPredDataset(name='ogbg-molhiv')\n",
        "split_idx = dataset.get_idx_split()\n",
        "train_idx, val_idx, test_idx=split_idx['train'], split_idx['valid'], split_idx['test']\n",
        "train_data = dataset[train_idx]\n",
        "val_data = dataset[val_idx]\n",
        "test_data = dataset[test_idx]\n",
        "\n",
        "test_loader = DataLoader(balanced_test_data, batch_size=32, shuffle=False)\n",
        "\n",
        "# Training loop with cross-validation\n",
        "\n",
        "train_loader = DataLoader(balanced_train_data, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(balanced_val_data, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Define the model and optimizer\n",
        "model = GNN(hidden_channels=64)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Early stopping variables\n",
        "best_val_acc = 0.0\n",
        "patience = 25  # Stop after no improvement for 25 epochs\n",
        "counter = 0\n",
        "\n",
        "# Training loop with validation and early stopping\n",
        "#  rappresenta un ciclo di addestramento del modello con validazione incrociata.\n",
        "for epoch in range(1, 101):  # Train for maximum 100 epochs\n",
        "#Train the model\n",
        "  train(model, optimizer, train_loader)\n",
        "        # Validate the model\n",
        "  val_acc, _, _ = validate(model, val_loader)\n",
        "\n",
        "  print(f'Fold [{fold+1}/{k_folds}] Epoch: {epoch}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "        # Check for improvement in validation accuracy\n",
        "  if val_acc > best_val_acc:\n",
        "    best_val_acc = val_acc\n",
        "    counter = 0  # Reset counter\n",
        "  else:\n",
        "    counter += 1  # No improvement, increase counter\n",
        "\n",
        "        # If no improvement for \"patience\" number of epochs, stop training\n",
        "  if counter >= patience:\n",
        "    print(f'Fold [{fold+1}/{k_folds}] Early stopping at epoch {epoch}')\n",
        "    break\n",
        "\n",
        "# Final evaluation on the test set\n",
        "# calcola e stampa l'accuratezza finale del modello utilizzando i dati di test.\n",
        "test_acc, all_test_predictions, all_test_labels = validate(model, test_loader)\n",
        "print(f'Final Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Print some predictions and labels\n",
        "print(\"Sample predictions and labels:\")\n",
        "for i in range(10):  # Print first 10 samples\n",
        "    print(f\"Prediction: {all_test_predictions[i]}, Label: {all_test_labels[i]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ndfc-PnAbMDM",
        "outputId": "0dd7c505-5878-47f5-8f30-c0dab73471f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold [1/5]\n",
            "Fold [1/5] Epoch: 1, Val Acc: 0.9639\n",
            "Fold [1/5] Epoch: 2, Val Acc: 0.9639\n",
            "Fold [1/5] Epoch: 3, Val Acc: 0.9639\n",
            "Fold [1/5] Epoch: 4, Val Acc: 0.9639\n",
            "Fold [1/5] Epoch: 5, Val Acc: 0.9639\n",
            "Fold [1/5] Epoch: 6, Val Acc: 0.9639\n",
            "Fold [1/5] Epoch: 7, Val Acc: 0.9639\n",
            "Fold [1/5] Epoch: 8, Val Acc: 0.9639\n",
            "Fold [1/5] Epoch: 9, Val Acc: 0.9639\n",
            "Fold [1/5] Epoch: 10, Val Acc: 0.9639\n",
            "Fold [1/5] Epoch: 11, Val Acc: 0.9639\n",
            "Fold [1/5] Epoch: 12, Val Acc: 0.9639\n",
            "Fold [1/5] Epoch: 13, Val Acc: 0.9639\n",
            "Fold [1/5] Epoch: 14, Val Acc: 0.9639\n",
            "Fold [1/5] Epoch: 15, Val Acc: 0.9639\n",
            "Fold [1/5] Epoch: 16, Val Acc: 0.9639\n",
            "Fold [1/5] Epoch: 17, Val Acc: 0.9639\n",
            "Fold [1/5] Epoch: 18, Val Acc: 0.9639\n",
            "Fold [1/5] Epoch: 19, Val Acc: 0.9639\n",
            "Fold [1/5] Epoch: 20, Val Acc: 0.9639\n",
            "Fold [1/5] Epoch: 21, Val Acc: 0.9639\n",
            "Fold [1/5] Epoch: 22, Val Acc: 0.9639\n",
            "Fold [1/5] Epoch: 23, Val Acc: 0.9639\n",
            "Fold [1/5] Epoch: 24, Val Acc: 0.9639\n",
            "Fold [1/5] Epoch: 25, Val Acc: 0.9639\n",
            "Fold [1/5] Epoch: 26, Val Acc: 0.9639\n",
            "Fold [1/5] Early stopping at epoch 26\n",
            "Fold [2/5]\n",
            "Fold [2/5] Epoch: 1, Val Acc: 0.9637\n",
            "Fold [2/5] Epoch: 2, Val Acc: 0.9637\n",
            "Fold [2/5] Epoch: 3, Val Acc: 0.9637\n",
            "Fold [2/5] Epoch: 4, Val Acc: 0.9637\n",
            "Fold [2/5] Epoch: 5, Val Acc: 0.9637\n",
            "Fold [2/5] Epoch: 6, Val Acc: 0.9637\n",
            "Fold [2/5] Epoch: 7, Val Acc: 0.9637\n",
            "Fold [2/5] Epoch: 8, Val Acc: 0.9637\n",
            "Fold [2/5] Epoch: 9, Val Acc: 0.9637\n",
            "Fold [2/5] Epoch: 10, Val Acc: 0.9637\n",
            "Fold [2/5] Epoch: 11, Val Acc: 0.9637\n",
            "Fold [2/5] Epoch: 12, Val Acc: 0.9637\n",
            "Fold [2/5] Epoch: 13, Val Acc: 0.9637\n",
            "Fold [2/5] Epoch: 14, Val Acc: 0.9637\n",
            "Fold [2/5] Epoch: 15, Val Acc: 0.9637\n",
            "Fold [2/5] Epoch: 16, Val Acc: 0.9637\n",
            "Fold [2/5] Epoch: 17, Val Acc: 0.9637\n",
            "Fold [2/5] Epoch: 18, Val Acc: 0.9637\n",
            "Fold [2/5] Epoch: 19, Val Acc: 0.9637\n",
            "Fold [2/5] Epoch: 20, Val Acc: 0.9637\n",
            "Fold [2/5] Epoch: 21, Val Acc: 0.9637\n",
            "Fold [2/5] Epoch: 22, Val Acc: 0.9637\n",
            "Fold [2/5] Epoch: 23, Val Acc: 0.9637\n",
            "Fold [2/5] Epoch: 24, Val Acc: 0.9637\n",
            "Fold [2/5] Epoch: 25, Val Acc: 0.9637\n",
            "Fold [2/5] Epoch: 26, Val Acc: 0.9637\n",
            "Fold [2/5] Early stopping at epoch 26\n",
            "Fold [3/5]\n",
            "Fold [3/5] Epoch: 1, Val Acc: 0.9709\n",
            "Fold [3/5] Epoch: 2, Val Acc: 0.9709\n",
            "Fold [3/5] Epoch: 3, Val Acc: 0.9709\n",
            "Fold [3/5] Epoch: 4, Val Acc: 0.9709\n",
            "Fold [3/5] Epoch: 5, Val Acc: 0.9709\n",
            "Fold [3/5] Epoch: 6, Val Acc: 0.9709\n",
            "Fold [3/5] Epoch: 7, Val Acc: 0.9709\n",
            "Fold [3/5] Epoch: 8, Val Acc: 0.9709\n",
            "Fold [3/5] Epoch: 9, Val Acc: 0.9709\n",
            "Fold [3/5] Epoch: 10, Val Acc: 0.9709\n",
            "Fold [3/5] Epoch: 11, Val Acc: 0.9709\n",
            "Fold [3/5] Epoch: 12, Val Acc: 0.9709\n",
            "Fold [3/5] Epoch: 13, Val Acc: 0.9709\n",
            "Fold [3/5] Epoch: 14, Val Acc: 0.9709\n",
            "Fold [3/5] Epoch: 15, Val Acc: 0.9709\n",
            "Fold [3/5] Epoch: 16, Val Acc: 0.9709\n",
            "Fold [3/5] Epoch: 17, Val Acc: 0.9709\n",
            "Fold [3/5] Epoch: 18, Val Acc: 0.9709\n",
            "Fold [3/5] Epoch: 19, Val Acc: 0.9709\n",
            "Fold [3/5] Epoch: 20, Val Acc: 0.9709\n",
            "Fold [3/5] Epoch: 21, Val Acc: 0.9709\n",
            "Fold [3/5] Epoch: 22, Val Acc: 0.9709\n",
            "Fold [3/5] Epoch: 23, Val Acc: 0.9709\n",
            "Fold [3/5] Epoch: 24, Val Acc: 0.9709\n",
            "Fold [3/5] Epoch: 25, Val Acc: 0.9709\n",
            "Fold [3/5] Epoch: 26, Val Acc: 0.9709\n",
            "Fold [3/5] Early stopping at epoch 26\n",
            "Fold [4/5]\n",
            "Fold [4/5] Epoch: 1, Val Acc: 0.9624\n",
            "Fold [4/5] Epoch: 2, Val Acc: 0.9624\n",
            "Fold [4/5] Epoch: 3, Val Acc: 0.9624\n",
            "Fold [4/5] Epoch: 4, Val Acc: 0.9624\n",
            "Fold [4/5] Epoch: 5, Val Acc: 0.9624\n",
            "Fold [4/5] Epoch: 6, Val Acc: 0.9624\n",
            "Fold [4/5] Epoch: 7, Val Acc: 0.9624\n",
            "Fold [4/5] Epoch: 8, Val Acc: 0.9624\n",
            "Fold [4/5] Epoch: 9, Val Acc: 0.9624\n",
            "Fold [4/5] Epoch: 10, Val Acc: 0.9624\n",
            "Fold [4/5] Epoch: 11, Val Acc: 0.9624\n",
            "Fold [4/5] Epoch: 12, Val Acc: 0.9624\n",
            "Fold [4/5] Epoch: 13, Val Acc: 0.9624\n",
            "Fold [4/5] Epoch: 14, Val Acc: 0.9624\n",
            "Fold [4/5] Epoch: 15, Val Acc: 0.9624\n",
            "Fold [4/5] Epoch: 16, Val Acc: 0.9624\n",
            "Fold [4/5] Epoch: 17, Val Acc: 0.9624\n",
            "Fold [4/5] Epoch: 18, Val Acc: 0.9624\n",
            "Fold [4/5] Epoch: 19, Val Acc: 0.9624\n",
            "Fold [4/5] Epoch: 20, Val Acc: 0.9624\n",
            "Fold [4/5] Epoch: 21, Val Acc: 0.9624\n",
            "Fold [4/5] Epoch: 22, Val Acc: 0.9624\n",
            "Fold [4/5] Epoch: 23, Val Acc: 0.9624\n",
            "Fold [4/5] Epoch: 24, Val Acc: 0.9624\n",
            "Fold [4/5] Epoch: 25, Val Acc: 0.9624\n",
            "Fold [4/5] Epoch: 26, Val Acc: 0.9624\n",
            "Fold [4/5] Early stopping at epoch 26\n",
            "Fold [5/5]\n",
            "Fold [5/5] Epoch: 1, Val Acc: 0.9636\n",
            "Fold [5/5] Epoch: 2, Val Acc: 0.9636\n",
            "Fold [5/5] Epoch: 3, Val Acc: 0.9636\n",
            "Fold [5/5] Epoch: 4, Val Acc: 0.9636\n",
            "Fold [5/5] Epoch: 5, Val Acc: 0.9636\n",
            "Fold [5/5] Epoch: 6, Val Acc: 0.9636\n",
            "Fold [5/5] Epoch: 7, Val Acc: 0.9636\n",
            "Fold [5/5] Epoch: 8, Val Acc: 0.9636\n",
            "Fold [5/5] Epoch: 9, Val Acc: 0.9636\n",
            "Fold [5/5] Epoch: 10, Val Acc: 0.9636\n",
            "Fold [5/5] Epoch: 11, Val Acc: 0.9636\n",
            "Fold [5/5] Epoch: 12, Val Acc: 0.9636\n",
            "Fold [5/5] Epoch: 13, Val Acc: 0.9636\n",
            "Fold [5/5] Epoch: 14, Val Acc: 0.9636\n",
            "Fold [5/5] Epoch: 15, Val Acc: 0.9636\n",
            "Fold [5/5] Epoch: 16, Val Acc: 0.9636\n",
            "Fold [5/5] Epoch: 17, Val Acc: 0.9636\n",
            "Fold [5/5] Epoch: 18, Val Acc: 0.9636\n",
            "Fold [5/5] Epoch: 19, Val Acc: 0.9636\n",
            "Fold [5/5] Epoch: 20, Val Acc: 0.9636\n",
            "Fold [5/5] Epoch: 21, Val Acc: 0.9636\n",
            "Fold [5/5] Epoch: 22, Val Acc: 0.9636\n",
            "Fold [5/5] Epoch: 23, Val Acc: 0.9636\n",
            "Fold [5/5] Epoch: 24, Val Acc: 0.9636\n",
            "Fold [5/5] Epoch: 25, Val Acc: 0.9636\n",
            "Fold [5/5] Epoch: 26, Val Acc: 0.9636\n",
            "Fold [5/5] Early stopping at epoch 26\n",
            "Final Test Accuracy: 0.9684\n",
            "Sample predictions and labels:\n",
            "Prediction: 0.0, Label: 0\n",
            "Prediction: 0.0, Label: 0\n",
            "Prediction: 0.0, Label: 0\n",
            "Prediction: 0.0, Label: 0\n",
            "Prediction: 0.0, Label: 0\n",
            "Prediction: 0.0, Label: 0\n",
            "Prediction: 0.0, Label: 0\n",
            "Prediction: 0.0, Label: 0\n",
            "Prediction: 0.0, Label: 0\n",
            "Prediction: 0.0, Label: 0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GCNConv, global_max_pool\n",
        "from torch.nn import Linear\n",
        "from ogb.graphproppred import PygGraphPropPredDataset\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Define Focal Loss\n",
        "# definisce una classe FocalLoss che implementa la perdita Focal Loss.\n",
        "class FocalLoss(torch.nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "\n",
        "# funzione forward definisce il passaggio in avanti (forward pass) della classe FocalLoss.\n",
        "#Durante la forward pass, viene calcolata la perdita Focal Loss tra gli input e i target.\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return torch.mean(F_loss)\n",
        "        elif self.reduction == 'sum':\n",
        "            return torch.sum(F_loss)\n",
        "        else:\n",
        "            return F_loss\n",
        "\n",
        "# Define a simple Graph Neural Network\n",
        "# definisce una classe GNNche implementa un modello di rete neurale per l'elaborazione dei grafi utilizzando\n",
        "# layer di convoluzione grafica (GCNConv) e un layer lineare finale (Linear).\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GNN, self).__init__()\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv4 = GCNConv(hidden_channels, hidden_channels)\n",
        "        #self.conv5 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin = Linear(hidden_channels, 1)  # Output will be 1 for binary classification\n",
        "\n",
        "\n",
        "# definisce il flusso dei dati attraverso il modello durante la fase di passaggio in avanti (forward pass).\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv4(x, edge_index)\n",
        "        x = x.relu()\n",
        "        #x = self.conv5(x, edge_index)\n",
        "        #x = x.relu()\n",
        "        x = global_max_pool(x, batch)  # Global pooling\n",
        "        x = self.lin(x).squeeze(1)  # Squeeze to remove extra dimension\n",
        "        return torch.sigmoid(x)  # Sigmoid for binary classification\n",
        "\n",
        "\n",
        "# implementa il ciclo di addestramento del modello durante l'addestramento.\n",
        "def train(model, optimizer, train_loader, loss_fn):\n",
        "    model.train()\n",
        "    for data in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x.float(), data.edge_index, data.batch)\n",
        "        target = data.y.view(-1, 1).float()  # Reshape to (batch_size, 1)\n",
        "        loss = loss_fn(out, target.squeeze(1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "# funzione di validazione che valuta le prestazioni del modello su un insieme di dati di validazione o test.\n",
        "def validate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            pred = model(data.x.float(), data.edge_index, data.batch)\n",
        "            pred = (pred > 0.5).float()  # Convert to binary predictions\n",
        "            correct += int((pred == data.y.view(-1)).sum())\n",
        "            total += data.y.size(0)\n",
        "            all_predictions.extend(pred.tolist())\n",
        "            all_labels.extend(data.y.view(-1).tolist())\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy, all_predictions, all_labels\n",
        "\n",
        "# Load dataset\n",
        "# carica un dataset per la classificazione di proprietà di grafi utilizzando PyTorch Geometric (PyG).\n",
        "dataset = PygGraphPropPredDataset(name='ogbg-molhiv')\n",
        "split_idx = dataset.get_idx_split()\n",
        "train_idx, val_idx, test_idx = split_idx['train'], split_idx['valid'], split_idx['test']\n",
        "train_data = dataset[train_idx]\n",
        "val_data = dataset[val_idx]\n",
        "test_data = dataset[test_idx]\n",
        "\n",
        "# Define cross-validation settings\n",
        "k_folds = 5\n",
        "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
        "\n",
        "# Training loop with cross-validation\n",
        "# implementa un loop di addestramento con validazione incrociata utilizzando la tecnica di focal loss.\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
        "    print(f\"Fold [{fold + 1}/{k_folds}]\")\n",
        "\n",
        "    # Subset the data for this fold\n",
        "    train_data = dataset[train_idx]\n",
        "    val_data = dataset[val_idx]\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Define the model and optimizer\n",
        "    model = GNN(hidden_channels=64)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    # Create Focal Loss with alpha=1, gamma=2\n",
        "    focal_loss = FocalLoss(alpha=1, gamma=2)\n",
        "\n",
        "    # Early stopping variables\n",
        "    best_val_acc = 0.0\n",
        "    patience = 25  # Stop after no improvement for 25 epochs\n",
        "    counter = 0\n",
        "\n",
        "    # Training loop with validation and early stopping\n",
        "    for epoch in range(1, 101):  # Train for maximum 100 epochs\n",
        "        # Train the model\n",
        "        train(model, optimizer, train_loader, focal_loss)\n",
        "\n",
        "        # Validate the model\n",
        "        val_acc, _, _ = validate(model, val_loader)\n",
        "\n",
        "        print(f'Fold [{fold + 1}/{k_folds}] Epoch: {epoch}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "        # Check for improvement in validation accuracy\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            counter = 0  # Reset counter\n",
        "        else:\n",
        "            counter += 1  # No improvement, increase counter\n",
        "\n",
        "        # If no improvement for \"patience\" number of epochs, stop training\n",
        "        if counter >= patience:\n",
        "            print(f'Fold [{fold + 1}/{k_folds}] Early stopping at epoch {epoch}')\n",
        "            break\n",
        "\n",
        "# Final evaluation on the test set\n",
        "test_acc, all_test_predictions, all_test_labels = validate(model, test_loader)\n",
        "print(f'Final Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Print some predictions and labels\n",
        "print(\"Sample predictions and labels:\")\n",
        "for i in range(10):  # Print first 10 samples\n",
        "    print(f\"Prediction: {all_test_predictions[i]}, Label: {all_test_labels[i]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBWtTaWJGhQf",
        "outputId": "81056592-b054-41f3-8f1b-90200a1271b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 5,  0,  4,  5,  3,  0,  2,  0,  0],\n",
            "        [ 5,  0,  4,  5,  2,  0,  2,  0,  0],\n",
            "        [ 5,  0,  3,  5,  0,  0,  1,  0,  1],\n",
            "        [ 7,  0,  2,  6,  0,  0,  1,  0,  1],\n",
            "        [28,  0,  4,  2,  0,  0,  5,  0,  1],\n",
            "        [ 7,  0,  2,  6,  0,  0,  1,  0,  1],\n",
            "        [ 5,  0,  3,  5,  0,  0,  1,  0,  1],\n",
            "        [ 5,  0,  4,  5,  2,  0,  2,  0,  0],\n",
            "        [ 5,  0,  4,  5,  3,  0,  2,  0,  0],\n",
            "        [ 5,  0,  4,  5,  2,  0,  2,  0,  1],\n",
            "        [ 7,  0,  2,  6,  0,  0,  1,  0,  1],\n",
            "        [ 5,  0,  3,  5,  0,  0,  1,  0,  1],\n",
            "        [ 5,  0,  4,  5,  2,  0,  2,  0,  0],\n",
            "        [ 5,  0,  4,  5,  3,  0,  2,  0,  0],\n",
            "        [ 5,  0,  4,  5,  2,  0,  2,  0,  1],\n",
            "        [ 5,  0,  3,  5,  0,  0,  1,  0,  1],\n",
            "        [ 5,  0,  4,  5,  2,  0,  2,  0,  0],\n",
            "        [ 5,  0,  4,  5,  3,  0,  2,  0,  0],\n",
            "        [ 7,  0,  2,  6,  0,  0,  1,  0,  1]])\n",
            "saliency tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Normalized Saliency Map:\n",
            "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
            "\n",
            "Sorted Indices (from most important to least important):\n",
            "tensor([10, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11,  0,  9,  8,  7,  6,  5,  4,\n",
            "         3,  2,  1])\n"
          ]
        }
      ],
      "source": [
        "# calcola la mappa di salienza dei nodi per un dato grafo.\n",
        "def node_compute_saliency_map(model, data):\n",
        "    model.eval()\n",
        "    data.x = data.x.float()  # Cast to float\n",
        "    data.x.requires_grad = True\n",
        "    logits = model(data.x, data.edge_index, data.batch)  # Pass data.batch as well\n",
        "    loss = logits[data.y.squeeze()]  # Assuming binary classification\n",
        "    loss.backward()\n",
        "\n",
        "    saliency_map = data.x.grad.abs().sum(dim=-1)  # Sum gradients across feature dimensions\n",
        "    return saliency_map\n",
        "\n",
        "\n",
        "# Example usage\n",
        "# prende il primo elemento dal set di dati di test, quindi chiama la funzione node_compute_saliency_map\n",
        "# per calcolare la mappa di salienza dei nodi per questo grafo.\n",
        "data = test_data[0]\n",
        "print(data.x)\n",
        "\n",
        "nodes_saliency_map = node_compute_saliency_map(model, data)\n",
        "print('saliency', nodes_saliency_map)\n",
        "\n",
        "# Normalize the saliency map\n",
        "#node_normalized_saliency_map = torch.abs(nodes_saliency_map) / torch.abs(nodes_saliency_map).max()\n",
        "\n",
        "# Get the indices of the features in descending order of importance\n",
        "#node_sorted_indices = torch.argsort(node_normalized_saliency_map, descending=True)\n",
        "\n",
        "# Print the normalized saliency map and sorted indices\n",
        "print(\"Normalized Saliency Map:\")\n",
        "print(node_normalized_saliency_map)\n",
        "\n",
        "print(\"\\nSorted Indices (from most important to least important):\")\n",
        "print(node_sorted_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vC3OAm9drw7N",
        "outputId": "9a02b17f-31c8-48a2-bf46-e56603041cbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalized Saliency Map:\n",
            "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
            "\n",
            "Sorted Indices (from most important to least important):\n",
            "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n"
          ]
        }
      ],
      "source": [
        "# calcola la mappa di salienza delle caratteristiche dei nodi per un grafo utilizzando il modello specificato.\n",
        "def feat_saliency_map_transposed(model, data):\n",
        "    model.eval()\n",
        "    x = data.x.float()  # Cast to float\n",
        "    x.requires_grad = True\n",
        "    # Forward pass through the model\n",
        "    logits = model(x, data.edge_index, data.batch)\n",
        "    loss = logits[data.y.squeeze()]\n",
        "    loss.backward()\n",
        "    # Manually compute the gradient with respect to the input tensor\n",
        "    gradient = x.grad.clone()  # Use x instead of data.x\n",
        "\n",
        "    # Transpose the node features tensor\n",
        "    x_transposed = x.transpose(0, 1)\n",
        "\n",
        "    # Transpose the gradient tensor\n",
        "    gradient_transposed = gradient.transpose(0, 1)\n",
        "\n",
        "    # Calculate the saliency map\n",
        "    saliency_map_transposed = gradient_transposed.abs().sum(dim=-1)  # Sum gradients across feature dimensions\n",
        "\n",
        "    return saliency_map_transposed\n",
        "\n",
        "# Calcola la mappa di salienza delle caratteristiche dei nodi utilizzando la funzione feat_saliency_map_transposed precedentemente definita.\n",
        "feat_saliency_map = feat_saliency_map_transposed(model,data)\n",
        "# Normalizza la mappa di salienza assolutizzando i valori e dividendo per il massimo valore assoluto\n",
        "feat_normalized_saliency_map = torch.abs(feat_saliency_map) / torch.abs(feat_saliency_map).max()\n",
        "\n",
        "# vengono ottenuti gli indici delle caratteristiche ordinati in ordine decrescente di importanza utilizzando torch.argsort\n",
        "feat_sorted_indices = torch.argsort(feat_normalized_saliency_map, descending=True)\n",
        "\n",
        "# Print the normalized saliency map and sorted indices\n",
        "print(\"Normalized Saliency Map:\")\n",
        "print(feat_normalized_saliency_map)\n",
        "\n",
        "print(\"\\nSorted Indices (from most important to least important):\")\n",
        "print(feat_sorted_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DmBh4fJAGBq3",
        "outputId": "ed2b334c-80bf-431d-cf86-4514e67b76a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data(edge_index=[2, 40], edge_attr=[40, 3], x=[19, 9], y=[1, 1], num_nodes=19)\n",
            "Normalized Saliency Map:\n",
            "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
            "\n",
            "Sorted Indices (from most important to least important):\n",
            "tensor([ 9, 18, 17, 16, 15, 14, 13, 12, 11, 10,  0,  8,  7,  6,  5,  4,  3,  2,\n",
            "         1])\n",
            "Normalized Saliency Map:\n",
            "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
            "\n",
            "Sorted Indices (from most important to least important):\n",
            "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n"
          ]
        },
        {
          "ename": "StopIteration",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-ead769aa7677>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nSorted Indices (from most important to least important):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_sorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m   \u001b[0mplot_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_normalized_saliency_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0mplot_hist_feat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_normalized_saliency_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-999f05ffa04d>\u001b[0m in \u001b[0;36mplot_graph\u001b[0;34m(data, nodes_saliency_map)\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspring_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Position nodes using a spring layout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m   \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_colors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfont_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfont_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"black\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Graph with Node Salience\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScalarMappable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Node Salience\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/networkx/drawing/nx_pylab.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(G, pos, ax, **kwds)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"with_labels\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"labels\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mdraw_networkx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis_off\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_if_interactive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/networkx/drawing/nx_pylab.py\u001b[0m in \u001b[0;36mdraw_networkx\u001b[0;34m(G, pos, arrows, with_labels, **kwds)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrawing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspring_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# default to spring layout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m     \u001b[0mdraw_networkx_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnode_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0mdraw_networkx_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0medge_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwith_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/networkx/drawing/nx_pylab.py\u001b[0m in \u001b[0;36mdraw_networkx_nodes\u001b[0;34m(G, pos, nodelist, node_size, node_color, node_shape, alpha, cmap, vmin, vmax, ax, linewidths, edgecolors, label, margins)\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m     node_collection = ax.scatter(\n\u001b[0m\u001b[1;32m    438\u001b[0m         \u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[1;32m   4600\u001b[0m             \u001b[0morig_edgecolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'edgecolor'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4601\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolors\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4602\u001b[0;31m             self._parse_scatter_color_args(\n\u001b[0m\u001b[1;32m   4603\u001b[0m                 \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4604\u001b[0m                 get_next_color_func=self._get_patches_for_fill.get_next_color)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36m_parse_scatter_color_args\u001b[0;34m(c, edgecolors, kwargs, xsize, get_next_color_func)\u001b[0m\n\u001b[1;32m   4398\u001b[0m             \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4399\u001b[0m             or (np.iterable(c) and len(c) > 0\n\u001b[0;32m-> 4400\u001b[0;31m                 and isinstance(cbook._safe_first_finite(c), str)))\n\u001b[0m\u001b[1;32m   4401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4402\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minvalid_shape_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_safe_first_finite\u001b[0;34m(obj, skip_nonfinite)\u001b[0m\n\u001b[1;32m   1713\u001b[0m                            \"support generators as input\")\n\u001b[1;32m   1714\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1715\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msafe_isfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mStopIteration\u001b[0m: "
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import networkx as nx\n",
        "for data in test_data: #itera attraverso ogni campione nel test_data\n",
        "  print(data)\n",
        "  nodes_saliency_map = node_compute_saliency_map(model, data) # r ogni campione, calcola la mappa di salienza dei nodi utilizzando la funzione node_compute_saliency_map(model, data).\n",
        "\n",
        "  # Normalizza la mappa di salienza dei nodi assolutizzando i valori e dividendo per il massimo valore assoluto.\n",
        "  node_normalized_saliency_map = torch.abs(nodes_saliency_map) / torch.abs(nodes_saliency_map).max()\n",
        "\n",
        "  # Ottiene gli indici dei nodi ordinati in ordine decrescente di importanza utilizzando torch.argsort.\n",
        "  node_sorted_indices = torch.argsort(node_normalized_saliency_map, descending=True)\n",
        "\n",
        "  # Print the normalized saliency map and sorted indices\n",
        "  print(\"Normalized Saliency Map:\")\n",
        "  print(node_normalized_saliency_map)\n",
        "\n",
        "  print(\"\\nSorted Indices (from most important to least important):\")\n",
        "  print(node_sorted_indices)\n",
        "\n",
        "  feat_saliency_map = feat_saliency_map_transposed(model,data) # calcola e normalizza la mappa di salienza delle caratteristiche trasposta utilizzando feat_saliency_map_transposed(model,data)\n",
        "  # Normalize the saliency map\n",
        "  feat_normalized_saliency_map = torch.abs(feat_saliency_map) / torch.abs(feat_saliency_map).max()\n",
        "\n",
        "  # Ottiene gli indici delle caratteristiche ordinati in ordine decrescente di importanza utilizzando torch.argsort.\n",
        "  feat_sorted_indices = torch.argsort(feat_normalized_saliency_map, descending=True)\n",
        "\n",
        "  # Stampa la mappa di salienza normalizzata e gli indici ordinati per le caratteristiche.\n",
        "  print(\"Normalized Saliency Map:\")\n",
        "  print(feat_normalized_saliency_map)\n",
        "\n",
        "# plotta il grafo e l'istogramma delle caratteristiche utilizzando le funzioni plot_graph() e plot_hist_feat() (che non sono definite nel codice fornito).\n",
        "  print(\"\\nSorted Indices (from most important to least important):\")\n",
        "  print(feat_sorted_indices)\n",
        "  plot_graph(data, node_normalized_saliency_map)\n",
        "  print('\\n')\n",
        "  plot_hist_feat(feat_normalized_saliency_map)\n",
        "  print('\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3XlSNFJF78r"
      },
      "source": [
        "#MAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJQkJ2hT7D1g"
      },
      "outputs": [],
      "source": [
        "# Define a simple Graph Neural Network\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch.nn import Linear, BatchNorm1d\n",
        "\n",
        "# definisce una rete neurale GNN\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GNN, self).__init__()\n",
        "        self.conv1 = GCNConv(9, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv4 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin = Linear(hidden_channels, 2)  # Output will be 2 classes\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv4(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = global_max_pool(x, batch)  # Global pooling\n",
        "        x = self.lin(x)\n",
        "        return x\n",
        "\n",
        "# è responsabile di addestrare il modello della rete neurale.\n",
        "def train(model, optimizer, train_loader):\n",
        "    model.train()\n",
        "    for data in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x.float(), data.edge_index, data.batch)\n",
        "        loss = F.cross_entropy(out, data.y.view(-1).long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# è utilizzata per valutare le prestazioni del modello su un insieme di dati di validazione o test\n",
        "def validate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            pred = model(data.x.float(), data.edge_index, data.batch)\n",
        "            pred = pred.argmax(dim=1)\n",
        "            correct += int((pred == data.y.view(-1)).sum())\n",
        "            total += data.y.size(0)\n",
        "            all_predictions.extend(pred.tolist())\n",
        "            all_labels.extend(data.y.view(-1).tolist())\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy, all_predictions, all_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTiHxH1z703r"
      },
      "outputs": [],
      "source": [
        "# calcola la mappa di salienza dei nodi rispetto alla classificazione del modello\n",
        "def node_compute_saliency_map(model, data):\n",
        "    model.eval()\n",
        "    data.x = data.x.float()  # Cast to float\n",
        "    data.x.requires_grad = True\n",
        "    logits = model(data.x, data.edge_index, data.batch)  # Pass data.batch as well\n",
        "    loss = logits[0, data.y.squeeze()]  # Assuming binary classification\n",
        "    loss.backward()\n",
        "\n",
        "    saliency_map = data.x.grad.abs().sum(dim=-1)  # Sum gradients across feature dimensions\n",
        "    return saliency_map\n",
        "\n",
        "# calcola la mappa di salienza delle feature dei nodi trasposta rispetto alla classificazione del modello\n",
        "def feat_saliency_map_transposed(model, data):\n",
        "    model.eval()\n",
        "    x = data.x.float()  # Cast to float\n",
        "    x.requires_grad = True\n",
        "    # Forward pass through the model\n",
        "    logits = model(x, data.edge_index, data.batch)\n",
        "    loss = logits[0, data.y.squeeze()]\n",
        "    loss.backward()\n",
        "    # Manually compute the gradient with respect to the input tensor\n",
        "    gradient = x.grad.clone()  # Use x instead of data.x\n",
        "\n",
        "    # Transpose the node features tensor\n",
        "    x_transposed = x.transpose(0, 1)\n",
        "\n",
        "    # Transpose the gradient tensor\n",
        "    gradient_transposed = gradient.transpose(0, 1)\n",
        "\n",
        "    # Calculate the saliency map\n",
        "    saliency_map_transposed = gradient_transposed.abs().sum(dim=-1)  # Sum gradients across feature dimensions\n",
        "\n",
        "    return saliency_map_transposed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjjC1LIQ8bh7"
      },
      "outputs": [],
      "source": [
        "# è progettata per visualizzare un grafo utilizzando la libreria NetworkX, con la colorazione dei nodi basata sulla mappa di salienza dei nodi\n",
        "from rdkit import Chem\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_graph(data, nodes_saliency_map):\n",
        "  tp = Chem.GetPeriodicTable()\n",
        "  G = nx.Graph()\n",
        "  elements = {}\n",
        "  for i in range(data[\"num_nodes\"]):\n",
        "      first_element_int = int(data.x[i][0].item())\n",
        "      elements[i] = tp.GetElementSymbol(first_element_int)\n",
        "      G.add_node(i)  # Redder for higher saliency\n",
        "\n",
        "  # Add edges\n",
        "  for i in range(data[\"edge_index\"].shape[1]):\n",
        "      source = data[\"edge_index\"][0, i].item()\n",
        "      target = data[\"edge_index\"][1, i].item()\n",
        "      G.add_edge(source, target)\n",
        "\n",
        "  # Get node colors for plotting\n",
        "  # Get node colors and sizes for plotting\n",
        "  node_colors = [nodes_saliency_map[i].item() for i in range(nodes_saliency_map.shape[0])]\n",
        "  node_sizes = [nodes_saliency_map[i].item() * 3000 for i in range(nodes_saliency_map.shape[0])]\n",
        "\n",
        "  # Plot the graph\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  pos = nx.spring_layout(G, seed=42)  # Position nodes using a spring layout\n",
        "  nx.draw(G, pos, with_labels=True, labels = elements, node_size=node_sizes, node_color=node_colors, cmap=plt.cm.Reds, font_size=10, font_color=\"black\")\n",
        "  plt.title(\"Graph with Node Salience\")\n",
        "  plt.colorbar(plt.cm.ScalarMappable(cmap=plt.cm.Reds), label=\"Node Salience\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lMLeFPSHYdH"
      },
      "outputs": [],
      "source": [
        "#  è progettata per visualizzare un istogramma che rappresenta l'importanza normalizzata delle caratteristiche (feature) del grafo\n",
        "def plot_hist_feat(feat_normalized_saliency_map):\n",
        "  # Create histogram\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.bar(range(len(feat_normalized_saliency_map.detach().numpy())), feat_normalized_saliency_map.detach().numpy(), color='skyblue')\n",
        "  plt.xlabel('Feature Index')\n",
        "  plt.ylabel('Normalized Importance')\n",
        "  plt.title('Normalized Feature Importance')\n",
        "  plt.xticks(range(len(feat_normalized_saliency_map)))  # Use feature index as x-ticks\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xdfW0lF6qi3",
        "outputId": "9fe54793-0941-464d-c960-5efd9d12278a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://snap.stanford.edu/ogb/data/graphproppred/csv_mol_download/hiv.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloaded 0.00 GB: 100%|██████████| 3/3 [00:00<00:00,  5.59it/s]\n",
            "Processing...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting dataset/hiv.zip\n",
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 41127/41127 [00:00<00:00, 62408.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 41127/41127 [00:04<00:00, 10262.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Done!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold [1/5]\n",
            "Fold [1/5] Epoch: 1, Val Acc: 0.5000\n",
            "Fold [1/5] Epoch: 2, Val Acc: 0.5000\n",
            "Fold [1/5] Epoch: 3, Val Acc: 0.5000\n",
            "Fold [1/5] Epoch: 4, Val Acc: 0.5000\n",
            "Fold [1/5] Epoch: 5, Val Acc: 0.5000\n",
            "Fold [1/5] Epoch: 6, Val Acc: 0.5000\n",
            "Fold [1/5] Epoch: 7, Val Acc: 0.5000\n",
            "Fold [1/5] Epoch: 8, Val Acc: 0.5000\n",
            "Fold [1/5] Epoch: 9, Val Acc: 0.5000\n",
            "Fold [1/5] Epoch: 10, Val Acc: 0.5000\n",
            "Fold [1/5] Epoch: 11, Val Acc: 0.5438\n",
            "Fold [1/5] Epoch: 12, Val Acc: 0.5690\n",
            "Fold [1/5] Epoch: 13, Val Acc: 0.5640\n",
            "Fold [1/5] Epoch: 14, Val Acc: 0.5640\n",
            "Fold [1/5] Epoch: 15, Val Acc: 0.5640\n",
            "Fold [1/5] Epoch: 16, Val Acc: 0.5640\n",
            "Fold [1/5] Epoch: 17, Val Acc: 0.5825\n",
            "Fold [1/5] Epoch: 18, Val Acc: 0.5640\n",
            "Fold [1/5] Epoch: 19, Val Acc: 0.5640\n",
            "Fold [1/5] Epoch: 20, Val Acc: 0.5640\n",
            "Fold [1/5] Epoch: 21, Val Acc: 0.5640\n",
            "Fold [1/5] Epoch: 22, Val Acc: 0.5640\n",
            "Fold [1/5] Epoch: 23, Val Acc: 0.5640\n",
            "Fold [1/5] Epoch: 24, Val Acc: 0.5640\n",
            "Fold [1/5] Epoch: 25, Val Acc: 0.5640\n",
            "Fold [1/5] Epoch: 26, Val Acc: 0.5640\n",
            "Fold [1/5] Epoch: 27, Val Acc: 0.5640\n",
            "Fold [1/5] Epoch: 28, Val Acc: 0.5640\n",
            "Fold [1/5] Epoch: 29, Val Acc: 0.5640\n",
            "Fold [1/5] Epoch: 30, Val Acc: 0.5640\n",
            "Fold [1/5] Epoch: 31, Val Acc: 0.5640\n",
            "Fold [1/5] Epoch: 32, Val Acc: 0.5640\n",
            "Fold [1/5] Epoch: 33, Val Acc: 0.5640\n",
            "Fold [1/5] Epoch: 34, Val Acc: 0.5640\n",
            "Fold [1/5] Epoch: 35, Val Acc: 0.5640\n",
            "Fold [1/5] Epoch: 36, Val Acc: 0.5640\n",
            "Fold [1/5] Epoch: 37, Val Acc: 0.5640\n",
            "Fold [1/5] Epoch: 38, Val Acc: 0.5640\n",
            "Fold [1/5] Epoch: 39, Val Acc: 0.5640\n",
            "Fold [1/5] Epoch: 40, Val Acc: 0.5640\n",
            "Fold [1/5] Epoch: 41, Val Acc: 0.5640\n",
            "Fold [1/5] Epoch: 42, Val Acc: 0.5640\n",
            "Fold [1/5] Early stopping at epoch 42\n",
            "Fold [2/5]\n",
            "Fold [2/5] Epoch: 1, Val Acc: 0.5134\n",
            "Fold [2/5] Epoch: 2, Val Acc: 0.6288\n",
            "Fold [2/5] Epoch: 3, Val Acc: 0.6187\n",
            "Fold [2/5] Epoch: 4, Val Acc: 0.6187\n",
            "Fold [2/5] Epoch: 5, Val Acc: 0.6171\n",
            "Fold [2/5] Epoch: 6, Val Acc: 0.5619\n",
            "Fold [2/5] Epoch: 7, Val Acc: 0.6421\n",
            "Fold [2/5] Epoch: 8, Val Acc: 0.5635\n",
            "Fold [2/5] Epoch: 9, Val Acc: 0.6572\n",
            "Fold [2/5] Epoch: 10, Val Acc: 0.6555\n",
            "Fold [2/5] Epoch: 11, Val Acc: 0.6254\n",
            "Fold [2/5] Epoch: 12, Val Acc: 0.6555\n",
            "Fold [2/5] Epoch: 13, Val Acc: 0.6572\n",
            "Fold [2/5] Epoch: 14, Val Acc: 0.6187\n",
            "Fold [2/5] Epoch: 15, Val Acc: 0.6054\n",
            "Fold [2/5] Epoch: 16, Val Acc: 0.6288\n",
            "Fold [2/5] Epoch: 17, Val Acc: 0.6589\n",
            "Fold [2/5] Epoch: 18, Val Acc: 0.6488\n",
            "Fold [2/5] Epoch: 19, Val Acc: 0.6572\n",
            "Fold [2/5] Epoch: 20, Val Acc: 0.6455\n",
            "Fold [2/5] Epoch: 21, Val Acc: 0.6706\n",
            "Fold [2/5] Epoch: 22, Val Acc: 0.6304\n",
            "Fold [2/5] Epoch: 23, Val Acc: 0.6421\n",
            "Fold [2/5] Epoch: 24, Val Acc: 0.6622\n",
            "Fold [2/5] Epoch: 25, Val Acc: 0.7007\n",
            "Fold [2/5] Epoch: 26, Val Acc: 0.6773\n"
          ]
        }
      ],
      "source": [
        "from rdkit import Chem\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GCNConv, global_max_pool\n",
        "from torch.nn import Linear\n",
        "from ogb.graphproppred import PygGraphPropPredDataset\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "\n",
        "# addestra e valida un modello utilizzando la tecnica della cross-validation\n",
        "def train_test_model():\n",
        "  tp = Chem.GetPeriodicTable()\n",
        "  # Load dataset\n",
        "  dataset = PygGraphPropPredDataset(name='ogbg-molhiv')\n",
        "  split_idx = dataset.get_idx_split()\n",
        "  train_idx, val_idx, test_idx=split_idx['train'], split_idx['valid'], split_idx['test']\n",
        "  train_data = dataset[train_idx]\n",
        "  val_data = dataset[val_idx]\n",
        "  test_data = dataset[test_idx]\n",
        "\n",
        "  # Define cross-validation settings\n",
        "  k_folds = 5\n",
        "  kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "  label_0_indices_te = (test_data.y == 0).nonzero(as_tuple=True)[0].reshape(-1)\n",
        "  label_1_indices_te = (test_data.y == 1).nonzero(as_tuple=True)[0].reshape(-1)\n",
        "\n",
        "  min_size_test = min(label_0_indices_te.size(0), label_1_indices_te.size(0))\n",
        "\n",
        "  label_0_indices_te = label_0_indices_te[:min_size_test]\n",
        "  label_1_indices_te = label_1_indices_te[:min_size_test]\n",
        "\n",
        "  balanced_indices_te = torch.cat([label_0_indices_te, label_1_indices_te])\n",
        "\n",
        "  balanced_test_data = test_data[balanced_indices_te]\n",
        "\n",
        "  test_loader = DataLoader(balanced_test_data, batch_size=32, shuffle=False)\n",
        "\n",
        "  # Training loop with cross-validation\n",
        "  for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
        "      print(f\"Fold [{fold+1}/{k_folds}]\")\n",
        "\n",
        "      # Subset the data for this fold\n",
        "      train_data = dataset[train_idx]\n",
        "      val_data = dataset[val_idx]\n",
        "\n",
        "      # Separate data by class labels\n",
        "      label_0_indices_tr = (train_data.y == 0).nonzero(as_tuple=True)[0].reshape(-1)\n",
        "      label_1_indices_tr = (train_data.y == 1).nonzero(as_tuple=True)[0].reshape(-1)\n",
        "\n",
        "      label_0_indices_vl = (val_data.y == 0).nonzero(as_tuple=True)[0].reshape(-1)\n",
        "      label_1_indices_vl = (val_data.y == 1).nonzero(as_tuple=True)[0].reshape(-1)\n",
        "\n",
        "      # Determine the size of the smaller class\n",
        "      min_size_train = min(label_0_indices_tr.size(0), label_1_indices_tr.size(0))\n",
        "      min_size_val = min(label_0_indices_vl.size(0), label_1_indices_vl.size(0))\n",
        "\n",
        "      # Sample equal number of elements from each class\n",
        "      label_0_indices_tr = label_0_indices_tr[:min_size_train]\n",
        "      label_1_indices_tr = label_1_indices_tr[:min_size_train]\n",
        "\n",
        "      label_0_indices_vl = label_0_indices_vl[:min_size_val]\n",
        "      label_1_indices_vl = label_1_indices_vl[:min_size_val]\n",
        "\n",
        "      # Concatenate the indices\n",
        "      balanced_indices_tr = torch.cat([label_0_indices_tr, label_1_indices_tr])\n",
        "      balanced_indices_vl = torch.cat([label_0_indices_vl, label_1_indices_vl])\n",
        "\n",
        "      # Create a new dataset with balanced samples\n",
        "      balanced_train_data = train_data[balanced_indices_tr]\n",
        "      balanced_val_data = val_data[balanced_indices_vl]\n",
        "\n",
        "      train_loader = DataLoader(balanced_train_data, batch_size=32, shuffle=True)\n",
        "      val_loader = DataLoader(balanced_val_data, batch_size=32, shuffle=False)\n",
        "\n",
        "      # Define the model and optimizer\n",
        "      model = GNN(hidden_channels=64)\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "      # Early stopping variables\n",
        "      best_val_acc = 0.0\n",
        "      patience = 25  # Stop after no improvement for 25 epochs\n",
        "      counter = 0\n",
        "\n",
        "      # Training loop with validation and early stopping\n",
        "      for epoch in range(1, 101):  # Train for maximum 100 epochs\n",
        "          #Train the model\n",
        "          train(model, optimizer, train_loader)\n",
        "\n",
        "          # Validate the model\n",
        "          val_acc, _, _ = validate(model, val_loader)\n",
        "\n",
        "          print(f'Fold [{fold+1}/{k_folds}] Epoch: {epoch}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "          # Check for improvement in validation accuracy\n",
        "          if val_acc > best_val_acc:\n",
        "              best_val_acc = val_acc\n",
        "              counter = 0  # Reset counter\n",
        "          else:\n",
        "              counter += 1  # No improvement, increase counter\n",
        "\n",
        "          # If no improvement for \"patience\" number of epochs, stop training\n",
        "          if counter >= patience:\n",
        "              print(f'Fold [{fold+1}/{k_folds}] Early stopping at epoch {epoch}')\n",
        "              break\n",
        "\n",
        "  # esegue la validazione finale del modello sul set di test bilanciato e calcola alcune metriche\n",
        "  # di valutazione della classificazione binaria, come l'accuratezza, la precisione, il richiamo (o recall) e il punteggio F1.\n",
        "  test_acc, all_test_predictions, all_test_labels = validate(model, test_loader)\n",
        "  print(f'Final Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "\n",
        "  # Convert predictions and labels to numpy arrays\n",
        "  all_test_predictions = np.array(all_test_predictions)\n",
        "  all_test_labels = np.array(all_test_labels)\n",
        "\n",
        "  # Calculate True Positives (TP), False Positives (FP),\n",
        "  # True Negatives (TN), False Negatives (FN)\n",
        "  TP = np.sum((all_test_predictions == 1) & (all_test_labels == 1))\n",
        "  FP = np.sum((all_test_predictions == 1) & (all_test_labels == 0))\n",
        "  TN = np.sum((all_test_predictions == 0) & (all_test_labels == 0))\n",
        "  FN = np.sum((all_test_predictions == 0) & (all_test_labels == 1))\n",
        "\n",
        "  print(\"True Positives (TP):\", TP)\n",
        "  print(\"False Positives (FP):\", FP)\n",
        "  print(\"True Negatives (TN):\", TN)\n",
        "  print(\"False Negatives (FN):\", FN)\n",
        "\n",
        "  # Calculate accuracy, precision, recall, and F1-score\n",
        "  accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
        "  precision = TP / (TP + FP)\n",
        "  recall = TP / (TP + FN)\n",
        "  f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "  print(\"Accuracy:\", accuracy)\n",
        "  print(\"Precision:\", precision)\n",
        "  print(\"Recall:\", recall)\n",
        "  print(\"F1-Score:\", f1_score)\n",
        "\n",
        "  return model, balanced_test_data\n",
        "\n",
        "model, balanced_test_data = train_test_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hzu2gs7KBV1r",
        "outputId": "76a14fc3-45cb-4421-de1e-45a8c7b74891"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([   0,    1,    2,  ..., 4110, 4111, 4112])\n",
            "tensor([  59,   62,   90,   98,  121,  140,  187,  196,  366,  395,  428,  440,\n",
            "         443,  444,  445,  446,  459,  460,  462,  463,  464,  467,  468,  469,\n",
            "         470,  597,  638,  685,  695,  697,  698,  699,  707,  730,  739,  740,\n",
            "         741,  742,  769,  772,  801,  826,  855,  858,  906,  946,  947,  948,\n",
            "         949,  950,  951,  956,  969,  975,  977, 1050, 1102, 1110, 1191, 1192,\n",
            "        1370, 1419, 1432, 1473, 1475, 1611, 1653, 1665, 1753, 1784, 1823, 1831,\n",
            "        2063, 2085, 2135, 2252, 2256, 2326, 2358, 2445, 2456, 2522, 2523, 2592,\n",
            "        2635, 2696, 2705, 2769, 2795, 2797, 2847, 2962, 3014, 3057, 3063, 3072,\n",
            "        3077, 3082, 3146, 3252, 3266, 3287, 3355, 3404, 3411, 3412, 3446, 3479,\n",
            "        3543, 3553, 3554, 3566, 3590, 3643, 3665, 3684, 3772, 3774, 3800, 3814,\n",
            "        3837, 3855, 3859, 3860, 3862, 3865, 4001, 4060, 4083, 4084])\n",
            "130\n",
            "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  60,  61,  63,  64,  65,  66,  67,  68,  69,  70,  71,\n",
            "         72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,  85,\n",
            "         86,  87,  88,  89,  91,  92,  93,  94,  95,  96,  97,  99, 100, 101,\n",
            "        102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115,\n",
            "        116, 117, 118, 119, 120, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n",
            "        131, 132, 133, 134])\n",
            "tensor([  59,   62,   90,   98,  121,  140,  187,  196,  366,  395,  428,  440,\n",
            "         443,  444,  445,  446,  459,  460,  462,  463,  464,  467,  468,  469,\n",
            "         470,  597,  638,  685,  695,  697,  698,  699,  707,  730,  739,  740,\n",
            "         741,  742,  769,  772,  801,  826,  855,  858,  906,  946,  947,  948,\n",
            "         949,  950,  951,  956,  969,  975,  977, 1050, 1102, 1110, 1191, 1192,\n",
            "        1370, 1419, 1432, 1473, 1475, 1611, 1653, 1665, 1753, 1784, 1823, 1831,\n",
            "        2063, 2085, 2135, 2252, 2256, 2326, 2358, 2445, 2456, 2522, 2523, 2592,\n",
            "        2635, 2696, 2705, 2769, 2795, 2797, 2847, 2962, 3014, 3057, 3063, 3072,\n",
            "        3077, 3082, 3146, 3252, 3266, 3287, 3355, 3404, 3411, 3412, 3446, 3479,\n",
            "        3543, 3553, 3554, 3566, 3590, 3643, 3665, 3684, 3772, 3774, 3800, 3814,\n",
            "        3837, 3855, 3859, 3860, 3862, 3865, 4001, 4060, 4083, 4084])\n",
            "PygGraphPropPredDataset(260)\n"
          ]
        }
      ],
      "source": [
        "# identifica gli indici delle istanze nel set di test test_data con etichetta 0 e 1 utilizzando la funzione nonzero. Questi indici\n",
        "#vengono memorizzati in label_0_indices_te e label_1_indices_te, rispettivamente.\n",
        "label_0_indices_te = (test_data.y == 0).nonzero(as_tuple=True)[0].reshape(-1)\n",
        "label_1_indices_te = (test_data.y == 1).nonzero(as_tuple=True)[0].reshape(-1)\n",
        "\n",
        "# Stampa gli indici delle istanze con etichetta 0 e 1.\n",
        "print(label_0_indices_te)\n",
        "print(label_1_indices_te)\n",
        "\n",
        "# Calcola la dimensione minima tra gli indici delle istanze con etichetta 0 e 1 utilizzando la funzione min.\n",
        "min_size_test = min(label_0_indices_te.size(0), label_1_indices_te.size(0))\n",
        "\n",
        "print(min_size_test)\n",
        "\n",
        "# limita gli indici delle istanze con etichetta 0 e 1 alla dimensione minima calcolata nella fase precedente utilizzando l'indicizzazione degli array.\n",
        "label_0_indices_te = label_0_indices_te[:min_size_test]\n",
        "label_1_indices_te = label_1_indices_te[:min_size_test]\n",
        "\n",
        "# stampa gli indici delle istanze con etichetta 0 e 1 dopo il ridimensionamento.\n",
        "print(label_0_indices_te)\n",
        "print(label_1_indices_te)\n",
        "\n",
        "# concatena gli indici delle istanze con etichetta 0 e 1 bilanciate utilizzando la funzione torch.cat e crea un nuovo\n",
        "#  set di dati di test bilanciato balanced_test_data.\n",
        "balanced_indices_te = torch.cat([label_0_indices_te, label_1_indices_te])\n",
        "\n",
        "# Stampa il nuovo set di dati di test bilanciato.\n",
        "balanced_test_data = test_data[balanced_indices_te]\n",
        "print(balanced_test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e122YHa2FedU",
        "outputId": "292bf895-09fa-4cc1-eeab-e6457d8a987c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([   0,    0,    1,  ...,    0, 4112,    0])\n",
            "tensor([  59,    0,   62,    0,   90,    0,   98,    0,  121,    0,  140,    0,\n",
            "         187,    0,  196,    0,  366,    0,  395,    0,  428,    0,  440,    0,\n",
            "         443,    0,  444,    0,  445,    0,  446,    0,  459,    0,  460,    0,\n",
            "         462,    0,  463,    0,  464,    0,  467,    0,  468,    0,  469,    0,\n",
            "         470,    0,  597,    0,  638,    0,  685,    0,  695,    0,  697,    0,\n",
            "         698,    0,  699,    0,  707,    0,  730,    0,  739,    0,  740,    0,\n",
            "         741,    0,  742,    0,  769,    0,  772,    0,  801,    0,  826,    0,\n",
            "         855,    0,  858,    0,  906,    0,  946,    0,  947,    0,  948,    0,\n",
            "         949,    0,  950,    0,  951,    0,  956,    0,  969,    0,  975,    0,\n",
            "         977,    0, 1050,    0, 1102,    0, 1110,    0, 1191,    0, 1192,    0,\n",
            "        1370,    0, 1419,    0, 1432,    0, 1473,    0, 1475,    0, 1611,    0,\n",
            "        1653,    0, 1665,    0, 1753,    0, 1784,    0, 1823,    0, 1831,    0,\n",
            "        2063,    0, 2085,    0, 2135,    0, 2252,    0, 2256,    0, 2326,    0,\n",
            "        2358,    0, 2445,    0, 2456,    0, 2522,    0, 2523,    0, 2592,    0,\n",
            "        2635,    0, 2696,    0, 2705,    0, 2769,    0, 2795,    0, 2797,    0,\n",
            "        2847,    0, 2962,    0, 3014,    0, 3057,    0, 3063,    0, 3072,    0,\n",
            "        3077,    0, 3082,    0, 3146,    0, 3252,    0, 3266,    0, 3287,    0,\n",
            "        3355,    0, 3404,    0, 3411,    0, 3412,    0, 3446,    0, 3479,    0,\n",
            "        3543,    0, 3553,    0, 3554,    0, 3566,    0, 3590,    0, 3643,    0,\n",
            "        3665,    0, 3684,    0, 3772,    0, 3774,    0, 3800,    0, 3814,    0,\n",
            "        3837,    0, 3855,    0, 3859,    0, 3860,    0, 3862,    0, 3865,    0,\n",
            "        4001,    0, 4060,    0, 4083,    0, 4084,    0])\n",
            "260\n",
            "tensor([  0,   0,   1,   0,   2,   0,   3,   0,   4,   0,   5,   0,   6,   0,\n",
            "          7,   0,   8,   0,   9,   0,  10,   0,  11,   0,  12,   0,  13,   0,\n",
            "         14,   0,  15,   0,  16,   0,  17,   0,  18,   0,  19,   0,  20,   0,\n",
            "         21,   0,  22,   0,  23,   0,  24,   0,  25,   0,  26,   0,  27,   0,\n",
            "         28,   0,  29,   0,  30,   0,  31,   0,  32,   0,  33,   0,  34,   0,\n",
            "         35,   0,  36,   0,  37,   0,  38,   0,  39,   0,  40,   0,  41,   0,\n",
            "         42,   0,  43,   0,  44,   0,  45,   0,  46,   0,  47,   0,  48,   0,\n",
            "         49,   0,  50,   0,  51,   0,  52,   0,  53,   0,  54,   0,  55,   0,\n",
            "         56,   0,  57,   0,  58,   0,  60,   0,  61,   0,  63,   0,  64,   0,\n",
            "         65,   0,  66,   0,  67,   0,  68,   0,  69,   0,  70,   0,  71,   0,\n",
            "         72,   0,  73,   0,  74,   0,  75,   0,  76,   0,  77,   0,  78,   0,\n",
            "         79,   0,  80,   0,  81,   0,  82,   0,  83,   0,  84,   0,  85,   0,\n",
            "         86,   0,  87,   0,  88,   0,  89,   0,  91,   0,  92,   0,  93,   0,\n",
            "         94,   0,  95,   0,  96,   0,  97,   0,  99,   0, 100,   0, 101,   0,\n",
            "        102,   0, 103,   0, 104,   0, 105,   0, 106,   0, 107,   0, 108,   0,\n",
            "        109,   0, 110,   0, 111,   0, 112,   0, 113,   0, 114,   0, 115,   0,\n",
            "        116,   0, 117,   0, 118,   0, 119,   0, 120,   0, 122,   0, 123,   0,\n",
            "        124,   0, 125,   0, 126,   0, 127,   0, 128,   0, 129,   0, 130,   0,\n",
            "        131,   0, 132,   0, 133,   0, 134,   0])\n",
            "tensor([  59,    0,   62,    0,   90,    0,   98,    0,  121,    0,  140,    0,\n",
            "         187,    0,  196,    0,  366,    0,  395,    0,  428,    0,  440,    0,\n",
            "         443,    0,  444,    0,  445,    0,  446,    0,  459,    0,  460,    0,\n",
            "         462,    0,  463,    0,  464,    0,  467,    0,  468,    0,  469,    0,\n",
            "         470,    0,  597,    0,  638,    0,  685,    0,  695,    0,  697,    0,\n",
            "         698,    0,  699,    0,  707,    0,  730,    0,  739,    0,  740,    0,\n",
            "         741,    0,  742,    0,  769,    0,  772,    0,  801,    0,  826,    0,\n",
            "         855,    0,  858,    0,  906,    0,  946,    0,  947,    0,  948,    0,\n",
            "         949,    0,  950,    0,  951,    0,  956,    0,  969,    0,  975,    0,\n",
            "         977,    0, 1050,    0, 1102,    0, 1110,    0, 1191,    0, 1192,    0,\n",
            "        1370,    0, 1419,    0, 1432,    0, 1473,    0, 1475,    0, 1611,    0,\n",
            "        1653,    0, 1665,    0, 1753,    0, 1784,    0, 1823,    0, 1831,    0,\n",
            "        2063,    0, 2085,    0, 2135,    0, 2252,    0, 2256,    0, 2326,    0,\n",
            "        2358,    0, 2445,    0, 2456,    0, 2522,    0, 2523,    0, 2592,    0,\n",
            "        2635,    0, 2696,    0, 2705,    0, 2769,    0, 2795,    0, 2797,    0,\n",
            "        2847,    0, 2962,    0, 3014,    0, 3057,    0, 3063,    0, 3072,    0,\n",
            "        3077,    0, 3082,    0, 3146,    0, 3252,    0, 3266,    0, 3287,    0,\n",
            "        3355,    0, 3404,    0, 3411,    0, 3412,    0, 3446,    0, 3479,    0,\n",
            "        3543,    0, 3553,    0, 3554,    0, 3566,    0, 3590,    0, 3643,    0,\n",
            "        3665,    0, 3684,    0, 3772,    0, 3774,    0, 3800,    0, 3814,    0,\n",
            "        3837,    0, 3855,    0, 3859,    0, 3860,    0, 3862,    0, 3865,    0,\n",
            "        4001,    0, 4060,    0, 4083,    0, 4084,    0])\n",
            "PygGraphPropPredDataset(520)\n"
          ]
        }
      ],
      "source": [
        "# identifica gli indici delle istanze con etichetta 0 e 1 nel set di test test_data utilizzando la funzione nonzero. Questa volta,\n",
        "# il parametro as_tuple è impostato su False, il che restituisce un tensore piatto invece di una tupla di tensori.\n",
        "label_0_indices_te = (test_data.y == 0).nonzero(as_tuple=False).reshape(-1)\n",
        "label_1_indices_te = (test_data.y == 1).nonzero(as_tuple=False).reshape(-1)\n",
        "\n",
        "# Stampa gli indici delle istanze con etichetta 0 e 1.\n",
        "print(label_0_indices_te)\n",
        "print(label_1_indices_te)\n",
        "\n",
        "# Calcola la dimensione minima tra gli indici delle istanze con etichetta 0 e 1 utilizzando la funzione min.\n",
        "min_size_test = min(label_0_indices_te.size(0), label_1_indices_te.size(0))\n",
        "\n",
        "print(min_size_test)\n",
        "\n",
        "# Limita gli indici delle istanze con etichetta 0 e 1 alla dimensione minima calcolata nella fase precedente utilizzando l'indicizzazione degli array.\n",
        "label_0_indices_te = label_0_indices_te[:min_size_test]\n",
        "label_1_indices_te = label_1_indices_te[:min_size_test]\n",
        "\n",
        "# Stampa gli indici delle istanze con etichetta 0 e 1 dopo il ridimensionamento.\n",
        "print(label_0_indices_te)\n",
        "print(label_1_indices_te)\n",
        "\n",
        "# Concatena gli indici delle istanze con etichetta 0 e 1 bilanciate utilizzando la funzione torch.cat e\n",
        "balanced_indices_te = torch.cat([label_0_indices_te, label_1_indices_te])\n",
        "\n",
        "balanced_test_data = test_data[balanced_indices_te] # #  crea un nuovo set di dati di test bilanciato balanced_test_data.\n",
        "print(balanced_test_data) # stampa il nuovo set di dati di test bilanciato"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaU_Z2SD8mdT"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "\n",
        "#  cicla attraverso il set di dati di test bilanciato balanced_test_data e calcola le mappe di salienza per i nodi e le feature per ciascuna istanza di dati.\n",
        "for data in balanced_test_data:\n",
        "  nodes_saliency_map = node_compute_saliency_map(model, data)\n",
        "\n",
        "  # Normalizza la mappa di salienza\n",
        "  node_normalized_saliency_map = torch.abs(nodes_saliency_map) / torch.abs(nodes_saliency_map).max()\n",
        "\n",
        "  # Ottieni gli indici delle caratteristiche in ordine decrescente di importanza\n",
        "  node_sorted_indices = torch.argsort(node_normalized_saliency_map, descending=True)\n",
        "\n",
        "  # Stampa la mappa di salienza normalizzata e gli indici ordinati\n",
        "  print(\"Normalized Saliency Map:\")\n",
        "  print(node_normalized_saliency_map)\n",
        "\n",
        "  print(\"\\nSorted Indices (from most important to least important):\")\n",
        "  print(node_sorted_indices)\n",
        "\n",
        "  feat_saliency_map = feat_saliency_map_transposed(model,data)\n",
        "  # Normalizza la saliency map\n",
        "  feat_normalized_saliency_map = torch.abs(feat_saliency_map) / torch.abs(feat_saliency_map).max()\n",
        "\n",
        "  # Ottieni gli indici delle caratteristiche in ordine decrescente di importanza\n",
        "  feat_sorted_indices = torch.argsort(feat_normalized_saliency_map, descending=True)\n",
        "\n",
        "  # Stampa la mappa di salienza normalizzata e gli indici ordinati\n",
        "  print(\"Normalized Saliency Map:\")\n",
        "  print(feat_normalized_saliency_map)\n",
        "\n",
        "  print(\"\\nSorted Indices (from most important to least important):\")\n",
        "  print(feat_sorted_indices)\n",
        "  plot_graph(data, node_normalized_saliency_map)\n",
        "  print('\\n')\n",
        "  plot_hist_feat(feat_normalized_saliency_map)\n",
        "  print('\\n')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}